"""
app.py
ì•½ì‚¬ ì±—ë´‡ RAG - Streamlit ë©”ì¸ ì•±.

ì‹¤í–‰: streamlit run app.py
"""

import os
import time
import asyncio

import streamlit as st
import torch
from dotenv import load_dotenv

# â”€â”€ CUDA ìµœì í™” í”Œë˜ê·¸ (ë¡œë“œ ì „ ì„¤ì •) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    torch.set_float32_matmul_precision("high")

from retriever import (
    load_embeddings,
    load_vector_db,
    load_reranker,
    build_bm25_retriever,
    get_ensemble_results,
    rerank_docs,
)
from generator import (
    build_context,
    generate_answer,
    verify_answer,
    self_correction_loop,
    evaluate_with_ragas,
    get_query_optimizer,
)
from processor import clear_gpu, get_clean_doc_text, get_gpu_status

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í˜ì´ì§€ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="ğŸ’Š ì•½ì‚¬ AI ì±—ë´‡",
    page_icon="ğŸ’Š",
    layout="wide",
    initial_sidebar_state="expanded",
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CSS í…Œë§ˆ (í”„ë¦¬ë¯¸ì—„ ì˜ë£Œ ë‹¤í¬ ìŠ¤íƒ€ì¼)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown(
    """
    <style>
    /* â”€â”€ Google Fonts â”€â”€ */
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');

    /* â”€â”€ ì „ì²´ ë°°ê²½ â”€â”€ */
    .stApp {
        background: linear-gradient(135deg, #0a0e1a 0%, #0d1527 50%, #0a1220 100%);
        font-family: 'Inter', sans-serif;
    }

    /* â”€â”€ ì‚¬ì´ë“œë°” â”€â”€ */
    [data-testid="stSidebar"] {
        background: linear-gradient(180deg, #0d1b2e 0%, #091424 100%);
        border-right: 1px solid rgba(59, 130, 246, 0.15);
    }
    [data-testid="stSidebar"] .stMarkdown h3 {
        color: #60a5fa;
        font-size: 0.8rem;
        font-weight: 600;
        letter-spacing: 0.08em;
        text-transform: uppercase;
        margin-bottom: 0.4rem;
    }

    /* â”€â”€ ë©”ì¸ í…ìŠ¤íŠ¸ ìƒ‰ìƒ â”€â”€ */
    .stMarkdown p, .stMarkdown li { color: #cbd5e1; }
    .stMarkdown h1, .stMarkdown h2, .stMarkdown h3 { color: #e2e8f0; }

    /* â”€â”€ ì±„íŒ… ë©”ì‹œì§€ â”€â”€ */
    [data-testid="stChatMessage"] {
        background: rgba(15, 23, 42, 0.8) !important;
        border: 1px solid rgba(59, 130, 246, 0.12);
        border-radius: 12px;
        padding: 0.75rem 1rem;
        margin-bottom: 0.5rem;
        backdrop-filter: blur(8px);
    }
    [data-testid="stChatMessage"][data-testid*="user"] {
        background: rgba(30, 58, 96, 0.5) !important;
        border-color: rgba(59, 130, 246, 0.25);
    }

    /* â”€â”€ íˆì–´ë¡œ í—¤ë” â”€â”€ */
    .hero-header {
        background: linear-gradient(135deg, rgba(37,99,235,0.15) 0%, rgba(16,185,129,0.08) 100%);
        border: 1px solid rgba(59,130,246,0.2);
        border-radius: 16px;
        padding: 1.5rem 2rem;
        margin-bottom: 1.5rem;
        backdrop-filter: blur(10px);
    }
    .hero-title {
        font-size: 2rem;
        font-weight: 700;
        background: linear-gradient(90deg, #60a5fa 0%, #34d399 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
        margin: 0;
    }
    .hero-subtitle {
        color: #94a3b8;
        font-size: 0.9rem;
        margin-top: 0.4rem;
    }
    .status-badge {
        display: inline-flex;
        align-items: center;
        gap: 0.3rem;
        background: rgba(16,185,129,0.15);
        border: 1px solid rgba(16,185,129,0.3);
        color: #34d399;
        font-size: 0.72rem;
        font-weight: 600;
        border-radius: 20px;
        padding: 0.15rem 0.65rem;
        margin-right: 0.4rem;
        margin-top: 0.6rem;
    }
    .badge-blue {
        background: rgba(59,130,246,0.1);
        border-color: rgba(59,130,246,0.3);
        color: #60a5fa;
    }
    .badge-purple {
        background: rgba(139,92,246,0.1);
        border-color: rgba(139,92,246,0.3);
        color: #a78bfa;
    }

    /* â”€â”€ PASS / FAIL ë°°ì§€ â”€â”€ */
    .verdict-pass {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        background: rgba(16,185,129,0.12);
        border: 1.5px solid rgba(16,185,129,0.4);
        color: #34d399;
        font-size: 1rem;
        font-weight: 700;
        border-radius: 10px;
        padding: 0.4rem 1.1rem;
        margin-top: 0.6rem;
        letter-spacing: 0.05em;
    }
    .verdict-fail {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        background: rgba(239,68,68,0.1);
        border: 1.5px solid rgba(239,68,68,0.35);
        color: #f87171;
        font-size: 1rem;
        font-weight: 700;
        border-radius: 10px;
        padding: 0.4rem 1.1rem;
        margin-top: 0.6rem;
        letter-spacing: 0.05em;
    }
    .verdict-corrected {
        font-size: 0.75rem;
        font-weight: 500;
        opacity: 0.8;
        margin-left: 0.3rem;
    }

    /* â”€â”€ ì†ŒìŠ¤ ë°°ì§€ â”€â”€ */
    .source-badge {
        font-size: 0.7rem;
        color: #60a5fa;
        background: rgba(59,130,246,0.12);
        border: 1px solid rgba(59,130,246,0.25);
        border-radius: 4px;
        padding: 2px 7px;
        margin-right: 5px;
        font-weight: 500;
    }
    .rank-badge {
        font-size: 0.7rem;
        color: #a78bfa;
        background: rgba(139,92,246,0.12);
        border: 1px solid rgba(139,92,246,0.25);
        border-radius: 4px;
        padding: 2px 7px;
        font-weight: 600;
    }

    /* â”€â”€ ì˜ˆì‹œ ì§ˆë¬¸ ì¹´ë“œ â”€â”€ */
    .example-label {
        color: #64748b;
        font-size: 0.72rem;
        font-weight: 600;
        letter-spacing: 0.06em;
        text-transform: uppercase;
        margin-bottom: 0.5rem;
    }

    /* â”€â”€ êµì • ë¡œê·¸ ì¹´ë“œ â”€â”€ */
    .correction-card {
        background: rgba(15, 23, 42, 0.6);
        border: 1px solid rgba(99,102,241,0.2);
        border-radius: 10px;
        padding: 0.8rem;
        margin-bottom: 0.5rem;
    }
    .round-label-pass { color: #34d399; font-size: 0.8rem; font-weight: 700; }
    .round-label-fail { color: #f87171; font-size: 0.8rem; font-weight: 700; }

    /* â”€â”€ Expander ìŠ¤íƒ€ì¼ â”€â”€ */
    [data-testid="stExpander"] {
        background: rgba(15, 23, 42, 0.6);
        border: 1px solid rgba(59, 130, 246, 0.15);
        border-radius: 10px;
    }

    /* â”€â”€ êµ¬ë¶„ì„  â”€â”€ */
    hr { border-color: rgba(59,130,246,0.15) !important; }

    /* â”€â”€ íƒ­ â”€â”€ */
    [data-testid="stTabs"] [role="tab"] {
        font-size: 0.85rem;
        font-weight: 500;
    }
    [data-testid="stTabs"] [role="tab"][aria-selected="true"] {
        color: #60a5fa;
        border-bottom-color: #60a5fa;
    }

    /* â”€â”€ VRAM ê³ ì • ìœ„ì ¯ â”€â”€ */
    .vram-widget {
        position: fixed;
        top: 56px;
        right: 16px;
        z-index: 9999;
        background: rgba(13, 27, 46, 0.92);
        border: 1px solid rgba(59, 130, 246, 0.25);
        border-radius: 12px;
        padding: 0.45rem 0.85rem;
        min-width: 210px;
        backdrop-filter: blur(10px);
        box-shadow: 0 4px 20px rgba(0,0,0,0.4);
    }
    .vram-title {
        font-size: 0.65rem;
        font-weight: 600;
        color: #60a5fa;
        letter-spacing: 0.07em;
        text-transform: uppercase;
        margin-bottom: 0.25rem;
    }
    .vram-bar-bg {
        width: 100%;
        height: 6px;
        background: rgba(255,255,255,0.08);
        border-radius: 4px;
        overflow: hidden;
        margin: 0.2rem 0;
    }
    .vram-bar-fill {
        height: 100%;
        border-radius: 4px;
        transition: width 0.4s ease;
    }
    .vram-text {
        font-size: 0.7rem;
        color: #94a3b8;
        margin-top: 0.15rem;
    }
    </style>
    """,
    unsafe_allow_html=True,
)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½ ë³€ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
openai_api_key = os.getenv("OPENAI_API_KEY", "")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‚¬ì´ë“œë°”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â”€â”€ í•˜ë“œì½”ë”©ëœ DB ê²½ë¡œ â”€â”€
db_path = "./chroma_db_combined_1771477980"

with st.sidebar:
    st.markdown(
        "<div style='text-align:center;padding:0.5rem 0 1rem;'>"
        "<span style='font-size:2rem;'>ğŸ’Š</span>"
        "<div style='color:#60a5fa;font-size:1.1rem;font-weight:700;margin-top:0.25rem;'>ì•½ì‚¬ AI</div>"
        "<div style='color:#475569;font-size:0.72rem;'>Pharmacist RAG System</div>"
        "</div>",
        unsafe_allow_html=True,
    )
    st.divider()

    # â”€â”€ ëª¨ë¸ ì„ íƒ â”€â”€
    st.markdown("### ğŸ¤– GPT ëª¨ë¸")
    gen_model = st.selectbox(
        "ë‹µë³€ ìƒì„± ëª¨ë¸",
        options=["gpt-5", "gpt-4o"],
        index=0,
        help="ë‹µë³€ ìƒì„±ì— ì‚¬ìš©í•  GPT ëª¨ë¸ (ê¸°ë³¸: gpt-5)",
        label_visibility="collapsed",
    )
    use_query_expansion = True  # í•­ìƒ í™œì„±í™”

    st.divider()

    # â”€â”€ ê²€ìƒ‰ ê°€ì¤‘ì¹˜ â”€â”€
    st.markdown("### ğŸ” ì•™ìƒë¸” ê°€ì¤‘ì¹˜")
    weight_bm25 = st.slider("BM25 (í‚¤ì›Œë“œ)", 0.0, 1.0, 0.8, 0.05, label_visibility="visible")
    weight_vector = round(1.0 - weight_bm25, 2)
    st.caption(f"ë²¡í„° ê²€ìƒ‰: **{weight_vector}** (ìë™)")

    top_k    = st.slider("Top-K (ìµœì¢… ë¬¸ì„œ ìˆ˜)", 3, 10, 5)
    ensemble_k = st.slider("ì•™ìƒë¸” í›„ë³´ ìˆ˜", 10, 50, 20)

    st.divider()

    # â”€â”€ ê³ ê¸‰ ì˜µì…˜ â”€â”€
    st.markdown("### ğŸ§ª ê³ ê¸‰ ì˜µì…˜")
    use_self_correction = st.checkbox("ğŸ”„ ìê¸° êµì • ë£¨í”„", value=True, help="FAIL ì‹œ ìµœëŒ€ 3íšŒ ì¬ì‹œë„")
    use_ragas           = st.checkbox("ğŸ“Š RAGAS í‰ê°€", value=False, help="ì¶”ê°€ API ë¹„ìš© ë°œìƒ")

    st.divider()

    # â”€â”€ ì˜ˆì‹œ ì§ˆë¬¸ â”€â”€
    st.markdown("### ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸")
    EXAMPLE_QUESTIONS = [
        "ëˆˆì´ ì¹¨ì¹¨í•œë° ë­ ë¨¹ìœ¼ë©´ ë ê¹Œìš”?",
        "íƒ€ì´ë ˆë†€ê³¼ ì´ë¶€í”„ë¡œíœ ê°™ì´ ë¨¹ì–´ë„ ë˜ë‚˜ìš”?",
        "ë£¨í…Œì¸ í•˜ë£¨ ë³µìš©ëŸ‰ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "ì„ì‚°ë¶€ê°€ ë¨¹ì–´ë„ ë˜ëŠ” ì˜ì–‘ì œê°€ ìˆë‚˜ìš”?",
        "ê°„ì— ì¢‹ì€ ì•½ì€ ì–´ë–¤ ê²Œ ìˆë‚˜ìš”?",
    ]
    if "pending_question" not in st.session_state:
        st.session_state.pending_question = ""

    for eq in EXAMPLE_QUESTIONS:
        if st.button(eq, key=f"ex_{eq}", use_container_width=True):
            st.session_state.pending_question = eq

    st.divider()

    # â”€â”€ ëŒ€í™” ì´ˆê¸°í™” â”€â”€
    if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True, type="secondary"):
        st.session_state.messages = []
        st.session_state.pending_question = ""
        st.rerun()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# VRAM ê³ ì • ìœ„ì ¯ (ìš°ì¸¡ ìƒë‹¨)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if torch.cuda.is_available():
    _gpu = get_gpu_status()
    _gpu_name = _gpu["name"]
    _vram_total = _gpu["total_gb"]
    _vram_used = _gpu["used_gb"]
    _vram_rsrvd = _gpu["reserved_gb"]
    _pct = min((_gpu["used_pct"] / 100.0), 1.0)
    _bar_color = "#34d399" if _pct < 0.6 else ("#facc15" if _pct < 0.85 else "#f87171")
    st.markdown(
        f"""
        <div class="vram-widget">
            <div class="vram-title">ğŸ’» GPU Â· {_gpu_name}</div>
            <div class="vram-bar-bg">
                <div class="vram-bar-fill" style="width:{_pct*100:.1f}%;background:{_bar_color};"></div>
            </div>
            <div class="vram-text">VRAM {_vram_used:.1f} / {_vram_total:.1f} GB &nbsp;Â·&nbsp; ì˜ˆì•½ {_vram_rsrvd:.1f} GB</div>
        </div>
        """,
        unsafe_allow_html=True,
    )
else:
    st.markdown(
        "<div class=\"vram-widget\"><div class=\"vram-title\">âš ï¸ GPU ë¯¸ì¸ì‹ â€“ CPU ëª¨ë“œ</div></div>",
        unsafe_allow_html=True,
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸ íˆì–´ë¡œ í—¤ë”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown(
    """
    <div class="hero-header">
      <div class="hero-title">ğŸ’Š ì•½ì‚¬ AI ì±—ë´‡</div>
      <div class="hero-subtitle">
        ì™¸ë¶€ ì˜ì•½í’ˆ ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ Â· ê·¼ê±° ì¤‘ì‹¬ ë‹µë³€ ìƒì„±
      </div>
      <div style="margin-top:0.6rem;">
        <span class="status-badge">âœ¦ BGE-M3-ko ì„ë² ë”©</span>
        <span class="status-badge badge-blue">âœ¦ BM25 + ë²¡í„° ì•™ìƒë¸”</span>
        <span class="status-badge badge-purple">âœ¦ CrossEncoder ë¦¬ë­í‚¹</span>
        <span class="status-badge">âœ¦ GPT ìê¸°ê²€ì¦</span>
      </div>
    </div>
    """,
    unsafe_allow_html=True,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DB ê²½ë¡œ ìœ íš¨ì„± í™•ì¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if not os.path.exists(db_path):
    st.error(
        f"âŒ ChromaDB ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: `{db_path}`\n\n"
        "ì‚¬ì´ë“œë°”ì—ì„œ ì˜¬ë°”ë¥¸ ChromaDB í´ë” ê²½ë¡œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.",
    )
    st.stop()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™” (ìµœì´ˆ 1íšŒ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "_initialized" not in st.session_state:
    with st.status("â³ ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™” ì¤‘...", expanded=True) as status:
        t_init = time.time()

        st.write("ğŸ§¬ BGE-M3 ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
        embeddings = load_embeddings()
        st.write(f"âœ”ï¸ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({time.time() - t_init:.1f}s)")

        t_db = time.time()
        st.write("ğŸ“¦ ChromaDB ë¡œë“œ ì¤‘...")
        vector_db = load_vector_db(db_path)
        st.write(f"âœ”ï¸ ChromaDB ë¡œë“œ ì™„ë£Œ ({time.time() - t_db:.1f}s)")

        t_re = time.time()
        st.write("âš¡ CrossEncoder ë¦¬ë­ì»¤ ë¡œë“œ ì¤‘...")
        reranker = load_reranker()
        st.write(f"âœ”ï¸ CrossEncoder ë¡œë“œ ì™„ë£Œ ({time.time() - t_re:.1f}s)")

        t_bm = time.time()
        st.write("ğŸš€ Kiwi + BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘ (ìµœì´ˆ 1íšŒ)...")
        bm25_retriever, kiwi = build_bm25_retriever(db_path)
        st.write(f"âœ”ï¸ BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ ({time.time() - t_bm:.1f}s)")

        status.update(
            label=f"âœ… ë¦¬ì†ŒìŠ¤ ì¤€ë¹„ ì™„ë£Œ! (ì „ì²´ {time.time() - t_init:.1f}s)",
            state="complete",
            expanded=False,
        )
    st.session_state._initialized = True
else:
    embeddings     = load_embeddings()
    vector_db      = load_vector_db(db_path)
    reranker       = load_reranker()
    bm25_retriever, kiwi = build_bm25_retriever(db_path)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# API í‚¤ ê²€ì¦
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if not openai_api_key:
    st.error("âŒ .env íŒŒì¼ì— OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", icon="ğŸ”‘")
    st.stop()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì´ì „ ëŒ€í™” í‘œì‹œ
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        if msg["role"] == "assistant" and "verdict" in msg:
            is_pass   = msg["verdict"]
            cor_rounds = msg.get("correction_rounds", 0)
            cor_txt   = f"<span class='verdict-corrected'>(êµì • {cor_rounds}íšŒ í›„)</span>" if cor_rounds else ""
            badge_cls = "verdict-pass" if is_pass else "verdict-fail"
            icon      = "âœ…" if is_pass else "âš ï¸"
            label     = "PASS" if is_pass else "FAIL"
            st.markdown(
                f"<span class='{badge_cls}'>{icon} ê²€ì¦ {label}{cor_txt}</span>",
                unsafe_allow_html=True,
            )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì˜ˆì‹œ ì§ˆë¬¸ ë²„íŠ¼ í´ë¦­ ì‹œ pending_questionì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
default_input = st.session_state.pop("pending_question", "") if "pending_question" in st.session_state else ""

user_query = st.chat_input(
    "ì•½ì— ëŒ€í•´ ê¶ê¸ˆí•œ ê²ƒì„ ë¬¼ì–´ë³´ì„¸ìš” (ì˜ˆ: ëˆˆì´ ì¹¨ì¹¨í•œë° ë­ ë¨¹ìœ¼ë©´ ë ê¹Œìš”?)",
    key="chat_input",
) or default_input

if user_query:
    # ì‚¬ìš©ì ë©”ì‹œì§€
    st.session_state.messages.append({"role": "user", "content": user_query})
    with st.chat_message("user"):
        st.markdown(user_query)

    with st.chat_message("assistant"):
        total_start = time.time()

        # â”€â”€ 1. Query Expansion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        query_optimizer = None
        if use_query_expansion:
            with st.spinner("ğŸª„ ì¿¼ë¦¬ í™•ì¥ ì¤‘..."):
                if "query_optimizer" not in st.session_state:
                    st.session_state.query_optimizer = get_query_optimizer(openai_api_key)
                if "search_keywords_cache" not in st.session_state:
                    st.session_state.search_keywords_cache = {}
                query_optimizer = st.session_state.query_optimizer
                cached_keywords = st.session_state.search_keywords_cache.get(user_query)
                cache_hit = cached_keywords is not None
                if cache_hit:
                    search_keywords = cached_keywords
                else:
                    optimize_prompt = (
                        f"ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ ì•½í•™ ê²€ìƒ‰ì— í•„ìš”í•œ í•µì‹¬ ì„±ë¶„ëª…, ì¦ìƒ, ì§ˆí™˜ í‚¤ì›Œë“œë§Œ ë½‘ì•„ ê³µë°±ìœ¼ë¡œ ë‚˜ì—´í•´ì¤˜: {user_query}"
                    )
                    try:
                        search_keywords = query_optimizer.invoke(optimize_prompt)
                    except Exception:
                        search_keywords = user_query
                    st.session_state.search_keywords_cache[user_query] = search_keywords
        else:
            search_keywords = None

        # â”€â”€ 2. ì•™ìƒë¸” ê²€ìƒ‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with st.spinner(f"ğŸ” ì•™ìƒë¸” ê²€ìƒ‰ ì¤‘ (BM25 {weight_bm25} / ë²¡í„° {weight_vector})..."):
            search_start = time.time()
            ensemble_docs = get_ensemble_results(
                query=user_query,
                kiwi=kiwi,
                bm25_retriever=bm25_retriever,
                vector_db=vector_db,
                query_optimizer=query_optimizer,
                search_keywords=search_keywords,
                k=ensemble_k,
                weight_bm25=weight_bm25,
                weight_vector=weight_vector,
            )
            search_elapsed = time.time() - search_start

        # â”€â”€ 3. CrossEncoder ë¦¬ë­í‚¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with st.spinner(f"âš¡ {len(ensemble_docs)}ê°œ ë¬¸ì„œ ë¦¬ë­í‚¹ ì¤‘..."):
            rerank_start = time.time()
            ranked_pairs: list[tuple[float, object]] = rerank_docs(
                query=user_query,
                docs=ensemble_docs,
                reranker=reranker,
                top_k=top_k,
                batch_size=32,
            )
            rerank_elapsed = time.time() - rerank_start

        # ì ìˆ˜Â·ë¬¸ì„œ ë¶„ë¦¬
        rerank_scores = [s for s, _ in ranked_pairs]
        final_docs    = [d for _, d in ranked_pairs]
        max_score     = max(rerank_scores) if rerank_scores else 1.0

        # â”€â”€ 4. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± + ë‹µë³€ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        context_text = build_context(final_docs)

        gen_start = time.time()
        stream_iter = asyncio.run(generate_answer(
            query=user_query,
            context_text=context_text,
            openai_api_key=openai_api_key,
            model=gen_model,
            stream=True,
        ))
        # í† í° ì‹¤ì‹œê°„ ë Œë”ë§
        final_answer = st.write_stream(stream_iter)
        gen_elapsed = time.time() - gen_start

        # â”€â”€ 5. ê²€ì¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with st.spinner("ğŸ§ ë‹µë³€ ê²€ì¦ ì¤‘..."):
            verify_start = time.time()
            verify_result = verify_answer(
                query=user_query,
                context_text=context_text,
                answer=final_answer,
                openai_api_key=openai_api_key,
                model="gpt-5.2",
            )
            verify_elapsed = time.time() - verify_start

        # â”€â”€ 6. ìê¸° êµì • ë£¨í”„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        correction_rounds = 0
        correction_logs: list[dict] = []
        if use_self_correction and "FAIL" in verify_result.upper():
            with st.spinner("ğŸ”„ ìê¸° êµì • ë£¨í”„ ì‹¤í–‰ ì¤‘ (ìµœëŒ€ 3íšŒ)..."):
                async def _run_correction():
                    res = {}
                    async for etype, val in self_correction_loop(
                        query=user_query,
                        context_text=context_text,
                        initial_answer=final_answer,
                        initial_verify_result=verify_result,
                        openai_api_key=openai_api_key,
                        gen_model=gen_model,
                        max_rounds=3,
                    ):
                        if etype == "done_loop":
                            res = val
                    return res
                
                loop_res = asyncio.run(_run_correction())
                if loop_res:
                    final_answer = loop_res["answer"]
                    verify_result = loop_res["verify_result"]
                    correction_rounds = loop_res["rounds"]
                    correction_logs = loop_res["logs"]

        # â”€â”€ ë‹µë³€ í‘œì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.markdown(final_answer)

        # PASS / FAIL ë°°ì§€
        is_passed   = "PASS" in verify_result.upper()
        badge_cls   = "verdict-pass" if is_passed else "verdict-fail"
        icon        = "âœ…" if is_passed else "âš ï¸"
        label       = "PASS" if is_passed else "FAIL"
        cor_txt     = (
            f"<span class='verdict-corrected'>(êµì • {correction_rounds}íšŒ í›„)</span>"
            if correction_rounds > 0 else ""
        )
        st.markdown(
            f"<span class='{badge_cls}'>{icon} ê²€ì¦ {label}{cor_txt}</span>",
            unsafe_allow_html=True,
        )

        total_elapsed = time.time() - total_start

        # â”€â”€ íƒ­ UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        tab_docs, tab_perf, tab_log, tab_ragas = st.tabs([
            f"ğŸ“„ ì°¸ê³  ë¬¸ì„œ ({len(final_docs)})",
            "ğŸ“Š ì„±ëŠ¥ ì§€í‘œ",
            f"ğŸ”„ êµì • ë¡œê·¸ ({max(correction_rounds, len(correction_logs))})",
            "ğŸ§ª RAGAS",
        ])

        # â”€â”€ íƒ­1: ì°¸ê³  ë¬¸ì„œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with tab_docs:
            for i, (score, doc) in enumerate(zip(rerank_scores, final_docs), 1):
                source = os.path.basename(doc.metadata.get("source", "Unknown"))
                content_preview = doc.page_content.replace("passage: ", "").replace("\n", " ")
                pct_score = min(score / max(max_score, 1e-6), 1.0) if max_score > 0 else 0.0
                pct_score = max(pct_score, 0.0)

                st.markdown(
                    f"**[{i}]** "
                    f"<span class='source-badge'>{source}</span>"
                    f"<span class='rank-badge'>ì ìˆ˜ {score:.3f}</span>",
                    unsafe_allow_html=True,
                )
                st.progress(float(pct_score), text=f"ê´€ë ¨ë„ {pct_score*100:.1f}%")
                st.caption(content_preview[:280] + "..." if len(content_preview) > 280 else content_preview)
                if i < len(final_docs):
                    st.divider()

        # â”€â”€ íƒ­2: ì„±ëŠ¥ ì§€í‘œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with tab_perf:
            c1, c2, c3, c4 = st.columns(4)
            c1.metric("ğŸ” ê²€ìƒ‰", f"{search_elapsed:.2f}s",
                      help=f"ì•™ìƒë¸” {len(ensemble_docs)}ê°œ â†’ {len(final_docs)}ê°œ")
            c2.metric("âš¡ ë¦¬ë­í‚¹", f"{rerank_elapsed:.2f}s",
                      help=f"CrossEncoder {len(ensemble_docs)}ê°œ ì²˜ë¦¬")
            c3.metric("âœï¸ ìƒì„±", f"{gen_elapsed:.2f}s")
            c4.metric("ğŸ§ ê²€ì¦", f"{verify_elapsed:.2f}s")
            st.metric("â±ï¸ ì „ì²´", f"{total_elapsed:.2f}s")

            st.divider()
            st.markdown("**ê²€ì¦ ìƒì„¸:**")
            st.text(verify_result)

        # â”€â”€ íƒ­3: êµì • ë¡œê·¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with tab_log:
            if not correction_logs:
                st.info("âœ… ìê¸° êµì • ì—†ì´ PASS íŒì •ì„ ë°›ì•˜ìŠµë‹ˆë‹¤." if is_passed
                        else "êµì • ë£¨í”„ë¥¼ ì‹¤í–‰í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ì‚¬ì´ë“œë°”ì—ì„œ í™œì„±í™”)")
            else:
                for log in correction_logs:
                    rno = log["round"]
                    r_pass = "PASS" in log["verify_result"].upper()
                    r_icon = "âœ…" if r_pass else "ğŸ”" if rno > 0 else "1ï¸âƒ£"
                    lbl_class = "round-label-pass" if r_pass else "round-label-fail"
                    with st.expander(
                        f"{'ROUND ' + str(rno) if rno > 0 else 'ROUND 0 (ì´ˆê¸°)'}  Â·  {'PASS' if r_pass else 'FAIL'}",
                        expanded=(rno == correction_rounds),
                    ):
                        st.markdown("**ë‹µë³€:**")
                        st.markdown(log["answer"])
                        st.markdown("**ê²€ì¦ ê²°ê³¼:**")
                        st.text(log["verify_result"])

        # â”€â”€ íƒ­4: RAGAS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with tab_ragas:
            if not use_ragas:
                st.info("ğŸ“Š ì‚¬ì´ë“œë°”ì—ì„œ **RAGAS í‰ê°€**ë¥¼ ì¼œë©´ faithfulnessÂ·answer_relevancy ì ìˆ˜ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            else:
                with st.spinner("ğŸ§ª RAGAS í‰ê°€ ì¤‘ (ì¶”ê°€ API í˜¸ì¶œ)..."):
                    ragas_scores = evaluate_with_ragas(
                        query=user_query,
                        answer=final_answer,
                        final_docs=final_docs,
                        embeddings=embeddings,
                        openai_api_key=openai_api_key,
                        eval_model=gen_model,
                    )
                if "error" in ragas_scores:
                    st.error(f"í‰ê°€ ì˜¤ë¥˜: {ragas_scores['error']}")
                else:
                    r1, r2 = st.columns(2)
                    faith = ragas_scores.get("faithfulness", 0.0)
                    relev = ragas_scores.get("answer_relevancy", 0.0)
                    r1.metric("âœ… Faithfulness", f"{faith:.4f}",
                              help="ë‹µë³€ì´ ë¬¸ì„œì— ì–¼ë§ˆë‚˜ ê·¼ê±°í•˜ëŠ”ì§€ (1.0 ìµœê³ )")
                    r2.metric("ğŸ¯ Answer Relevancy", f"{relev:.4f}",
                              help="ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆëŠ”ì§€ (1.0 ìµœê³ )")
                    st.progress(float(faith), text=f"Faithfulness {faith*100:.1f}%")
                    st.progress(float(relev), text=f"Answer Relevancy {relev*100:.1f}%")

        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        clear_gpu()

        # â”€â”€ íˆìŠ¤í† ë¦¬ ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        doc_snapshots = [
            {
                "source": doc.metadata.get("source", "Unknown"),
                "content": doc.page_content.replace("passage: ", ""),
            }
            for doc in final_docs
        ]
        st.session_state.messages.append({
            "role": "assistant",
            "content": final_answer,
            "verdict": is_passed,
            "correction_rounds": correction_rounds,
            "docs": doc_snapshots,
        })
