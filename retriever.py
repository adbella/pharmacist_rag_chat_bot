"""
retriever.py
BGE-M3 ì„ë² ë”©, ChromaDB, BM25, CrossEncoder ë¡œë“œ ë° ì•™ìƒë¸” ê²€ìƒ‰ í•¨ìˆ˜.

ëª¨ë“  ë¬´ê±°ìš´ ë¦¬ì†ŒìŠ¤ëŠ” @st.cache_resourceë¡œ ìºì‹±í•˜ì—¬
Streamlit ì¬ì‹¤í–‰ ì‹œ ì¬ë¡œë“œë¥¼ ë§‰ìŠµë‹ˆë‹¤.
"""

import time
import logging
import os
from concurrent.futures import ThreadPoolExecutor

import streamlit as st
import torch
from decorators import conditional_cache_resource
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document
from sentence_transformers import CrossEncoder

from processor import (
    clear_gpu,
    get_kiwi_tokenizer,
    tokenize_corpus,
    tokenize_query,
)
from generator import _retry_api_call
from external_pharma_api import fetch_external_pharma_docs

logger = logging.getLogger(__name__)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GPU/CPU ìë™ ì„ íƒ í—¬í¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _get_device() -> str:
    return "cuda" if torch.cuda.is_available() else "cpu"


def _configure_gpu() -> None:
    """RTX 2070 ìµœì  CUDA ì„¤ì •."""
    if torch.cuda.is_available():
        # Tensor Core í™œìš© (Turing ì´ìƒ)
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        # matmul ì •ë°€ë„ â€“ TF32 í—ˆìš©ìœ¼ë¡œ ì†ë„ í–¥ìƒ
        torch.set_float32_matmul_precision("high")


# ì•± ê¸°ë™ ì‹œ 1íšŒ ì‹¤í–‰
_configure_gpu()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¦¬ì†ŒìŠ¤ ë¡œë” (@st.cache_resource â†’ ìµœì´ˆ 1íšŒë§Œ ì‹¤í–‰)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@conditional_cache_resource(show_spinner="ğŸ§¬ BGE-M3 ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
def load_embeddings() -> HuggingFaceEmbeddings:
    """
    BGE-M3-ko ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    GPU: fp16 + batch_size 128ë¡œ ê³ ì† ì¸ì½”ë”©.
    CPU: fp32 í´ë°±.
    """
    device = _get_device()
    embedding_model_name = os.getenv("EMBEDDING_MODEL_NAME", "dragonkue/BGE-m3-ko")

    # GPUë©´ fp16, CPUë©´ fp32
    encode_kwargs: dict = {
        "normalize_embeddings": True,
        "batch_size": 128 if device == "cuda" else 32,
    }
    # langchain_huggingfaceëŠ” model_kwargsë¥¼ **kwargsë¡œ SentenceTransformerì— ì „ë‹¬.
    # SentenceTransformer 5.xëŠ” torch_dtypeì„ ìµœìƒìœ„ kwargë¡œ ë°›ì§€ ì•Šê³ ,
    # ìì²´ model_kwargs íŒŒë¼ë¯¸í„°(HuggingFace AutoModelì— ì „ë‹¬)ë¡œ ë°›ì•„ì•¼ í•¨.
    model_kwargs: dict = {"device": device}
    if device == "cuda":
        # SentenceTransformer(model_name, device="cuda", model_kwargs={"torch_dtype": fp16})
        model_kwargs["model_kwargs"] = {"dtype": torch.float16}

    embeddings = HuggingFaceEmbeddings(
        model_name=embedding_model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs,
    )
    logger.info(
        "[Embeddings] ë¡œë“œ ì™„ë£Œ (model=%s, device=%s, fp16=%s)",
        embedding_model_name,
        device,
        device == "cuda",
    )
    return embeddings


@conditional_cache_resource(show_spinner="ğŸ“¦ ChromaDB ë²¡í„° DB ë¡œë“œ ì¤‘...")
def load_vector_db(db_path: str) -> Chroma:
    """
    ë¡œì»¬ ChromaDBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        db_path: persist_directory ê²½ë¡œ (ì˜ˆ: './chroma_db_combined_stored')
    """
    embeddings = load_embeddings()
    vector_db = Chroma(
        persist_directory=db_path,
        embedding_function=embeddings,
    )
    count = vector_db._collection.count()
    logger.info("[ChromaDB] ë¡œë“œ ì™„ë£Œ: %d ë¬¸ì„œ", count)
    return vector_db


@conditional_cache_resource(show_spinner="âš¡ CrossEncoder ë¦¬ë­ì»¤ ë¡œë“œ ì¤‘...")
def load_reranker() -> CrossEncoder:
    """
    BAAI/bge-reranker-v2-m3 ë¦¬ë­í‚¹ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    GPU: fp16 + max_length 768.
    """
    device = _get_device()
    reranker_model_name = os.getenv("RERANKER_MODEL_NAME", "BAAI/bge-reranker-v2-m3")
    reranker = CrossEncoder(
        reranker_model_name,
        device=device,
        max_length=768,          # 512 â†’ 768: ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ ë°˜ì˜
        model_kwargs={
            "dtype": torch.float16 if device == "cuda" else torch.float32,
        },
    )
    logger.info(
        "[CrossEncoder] ë¡œë“œ ì™„ë£Œ (model=%s, device=%s, fp16=%s)",
        reranker_model_name,
        device,
        device == "cuda",
    )
    return reranker


@conditional_cache_resource(show_spinner="ğŸš€ BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘ (ìµœì´ˆ 1íšŒ, ì•½ 1~2ë¶„ ì†Œìš”)...")
def build_bm25_retriever(db_path: str) -> tuple[BM25Retriever, object]:
    """
    ChromaDBì—ì„œ ì „ì²´ ë¬¸ì„œë¥¼ ê°€ì ¸ì™€ Kiwi í˜•íƒœì†Œ ë¶„ì„ í›„ BM25 ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.

    Args:
        db_path: ChromaDB ê²½ë¡œ

    Returns:
        (bm25_retriever, kiwi) íŠœí”Œ
    """
    t0 = time.time()
    vector_db = load_vector_db(db_path)

    # 1. ì „ì²´ DB ë°ì´í„° ì¶”ì¶œ
    all_data = vector_db.get()
    original_docs = [
        Document(page_content=txt, metadata=meta)
        for txt, meta in zip(all_data["documents"], all_data["metadatas"])
    ]
    logger.info("[BM25] DB ì¶”ì¶œ ì™„ë£Œ: %d ë¬¸ì„œ, %.2fs", len(original_docs), time.time() - t0)

    # 2. Kiwi ì´ˆê¸°í™” ë° í˜•íƒœì†Œ ë¶„ì„ (ë©€í‹°ì½”ì–´ ë°°ì¹˜ ì²˜ë¦¬)
    t1 = time.time()
    kiwi = get_kiwi_tokenizer()
    tokenized_corpus = tokenize_corpus(original_docs, kiwi)
    logger.info("[BM25] Kiwi í† í¬ë‚˜ì´ì§• ì™„ë£Œ: %.2fs", time.time() - t1)

    # 3. BM25 ë¦¬íŠ¸ë¦¬ë²„ êµ¬ì¶• (ì´ë¯¸ í† í°í™”ëœ í…ìŠ¤íŠ¸ë¥¼ ê³µë°± ì¡°ì¸í•´ ì „ë‹¬,
    #    preprocess_funcìœ¼ë¡œ ì¬ë¶„ë¦¬í•˜ì—¬ rank_bm25ì˜ ë‚´ë¶€ ì½”í¼ìŠ¤ì— ì ì¬)
    t2 = time.time()
    joined_texts = [" ".join(tokens) for tokens in tokenized_corpus]
    bm25_retriever = BM25Retriever.from_texts(
        joined_texts,
        metadatas=[doc.metadata for doc in original_docs],
        preprocess_func=lambda x: x.split(),  # ê³µë°± split â†’ í† í° ë¦¬ìŠ¤íŠ¸ ë³µì›
        k=35,
    )
    # ì›ë³¸ Document ë³µì› (BM25Retrieverê°€ ë°˜í™˜í•  ë•Œ ì‚¬ìš©)
    bm25_retriever.docs = original_docs
    logger.info("[BM25] ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: %.2fs", time.time() - t2)
    logger.info("[BM25] ì „ì²´ ì´ˆê¸°í™” ì†Œìš”: %.2fs", time.time() - t0)

    return bm25_retriever, kiwi


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì•™ìƒë¸” ê²€ìƒ‰ (BM25 + Vector, RRF í†µí•©) â€” ë³‘ë ¬ ì‹¤í–‰ ë²„ì „
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_ensemble_results(
    query: str,
    kiwi,
    bm25_retriever: BM25Retriever,
    vector_db: Chroma,
    query_optimizer=None,
    search_keywords: str | None = None,
    k: int = 20,
    weight_bm25: float = 0.8,
    weight_vector: float = 0.2,
    use_external_api: bool = False,
    external_provider: str = "openfda",
    external_top_k: int = 4,
    external_timeout_sec: float = 8.0,
    weight_external: float = 0.2,
) -> list[Document]:
    """
    ì½œë© ì›ë³¸ ì½”ë“œë¥¼ ì°¸ê³ í•œ ì§€ëŠ¥í˜• ì•™ìƒë¸” ê²€ìƒ‰ ì‹¤í–‰.
    - MMR ê²€ìƒ‰ìœ¼ë¡œ ë‹¤ì–‘ì„± í™•ë³´
    - íŠ¹ì • íƒœê·¸(N, X, S, VA) ê¸°ë°˜ í‚¤ì›Œë“œ í•„í„°ë§
    """
    # 1. Query Expansion (Query Expansion)
    if search_keywords is None:
        if query_optimizer is not None:
            optimize_prompt = (
                f"ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ ì•½í•™ ê²€ìƒ‰ì— í•„ìš”í•œ í•µì‹¬ ì„±ë¶„ëª…, ì¦ìƒ, ì§ˆí™˜ í‚¤ì›Œë“œë§Œ ë½‘ì•„ ê³µë°±ìœ¼ë¡œ ë‚˜ì—´í•´ì¤˜: {query}"
            )
            try:
                search_keywords = _retry_api_call(query_optimizer.invoke, optimize_prompt)
            except Exception:
                search_keywords = query
        else:
            search_keywords = query

    # 2. í‚¤ì›Œë“œ í† í°í™” (Colab ì›ë³¸ ì½”ë“œ ê¸°ì¤€ íƒœê·¸ í•„í„°ë§)
    query_tokens = [
        t.form
        for t in kiwi.tokenize(search_keywords)
        if t.tag.startswith(('N', 'X', 'S', 'VA'))
    ]
    query_text = " ".join(query_tokens)

    # 3. ê° ê²€ìƒ‰ ìˆ˜í–‰ (BM25 + MMR ë²¡í„° ê²€ìƒ‰ + ì™¸ë¶€ API ë³‘ë ¬ ì²˜ë¦¬)
    def _bm25_search():
        # BM25 ë¦¬íŠ¸ë¦¬ë²„ë¥¼ í†µí•´ í‚¤ì›Œë“œ ê²€ìƒ‰
        return bm25_retriever.invoke(query_text)

    def _vector_search():
        # MMR(Maximum Marginal Relevance) ê²€ìƒ‰ìœ¼ë¡œ ê²°ê³¼ì˜ ë‹¤ì–‘ì„± í™•ë³´
        # fetch_k: í›„ë³´êµ° ì¶”ì¶œ ìˆ˜, lambda_mult: ë‹¤ì–‘ì„± ì§€ìˆ˜ (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë‹¤ì–‘)
        return vector_db.max_marginal_relevance_search(
            f"query: {query}", 
            k=k, 
            fetch_k=min(k * 2, 40), 
            lambda_mult=0.5
        )

    def _external_search():
        if not use_external_api:
            return []
        try:
            return fetch_external_pharma_docs(
                query=query,
                provider=external_provider,
                top_k=external_top_k,
                timeout_sec=external_timeout_sec,
            )
        except Exception as e:
            logger.warning("[ExternalAPI] ë¬¸ì„œ ì¡°íšŒ ì‹¤íŒ¨: %s", e)
            return []

    with ThreadPoolExecutor(max_workers=3) as executor:
        future_bm25 = executor.submit(_bm25_search)
        future_vector = executor.submit(_vector_search)
        future_external = executor.submit(_external_search)
        keyword_docs = future_bm25.result()
        vector_docs = future_vector.result()
        external_docs = future_external.result()

    # 4. RRF(Reciprocal Rank Fusion) í†µí•© ë° ë°ì´í„° íƒ€ì… êµì •
    doc_scores: dict[str, float] = {}
    doc_map: dict[str, Document] = {}

    def _process_results(docs, weight: float) -> None:
        for rank, doc in enumerate(docs):
            if isinstance(doc, Document):
                content = doc.page_content
                doc_obj = doc
            elif isinstance(doc, list):
                content = " ".join(doc)
                doc_obj = Document(page_content=content, metadata={"source": "BM25_Index"})
            else:
                content = str(doc)
                doc_obj = Document(page_content=content, metadata={"source": "Unknown"})

            if content not in doc_scores:
                doc_map[content] = doc_obj
                doc_scores[content] = 0.0
            doc_scores[content] += weight * (1 / (rank + 60))

    _process_results(keyword_docs, weight_bm25)
    _process_results(vector_docs, weight_vector)
    _process_results(external_docs, max(0.0, float(weight_external)))

    if use_external_api:
        logger.info(
            "[ExternalAPI] provider=%s, fetched=%d, weight=%.2f",
            external_provider,
            len(external_docs),
            weight_external,
        )

    # 5. ìµœì¢… ê²°ê³¼ ì •ë ¬ ë° kê°œ ë°˜í™˜
    sorted_items = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
    return [doc_map[content] for content, score in sorted_items[:k]]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CrossEncoder ë¦¬ë­í‚¹ â€” GPU fp16 ê³ ì† ì²˜ë¦¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def rerank_docs(
    query: str,
    docs: list[Document],
    reranker: CrossEncoder,
    top_k: int = 5,
    batch_size: int = 64,   # 32 â†’ 64: RTX 2070 8GB ê¸°ì¤€ ìµœì ê°’
) -> list[tuple[float, Document]]:
    """
    CrossEncoderë¡œ ì•™ìƒë¸” ê²°ê³¼ë¥¼ ì •ë°€ ë¦¬ë­í‚¹í•©ë‹ˆë‹¤.
    GPU fp16 ëª¨ë“œì—ì„œ batch_size=64ë¡œ ì²˜ë¦¬ëŸ‰ ìµœëŒ€í™”.

    Args:
        query:      ì‚¬ìš©ì ì›ë³¸ ì§ˆë¬¸
        docs:       ì•™ìƒë¸” ê²€ìƒ‰ ê²°ê³¼ Document ë¦¬ìŠ¤íŠ¸
        reranker:   ë¡œë“œëœ CrossEncoder ì¸ìŠ¤í„´ìŠ¤
        top_k:      ìµœì¢… ë°˜í™˜ ë¬¸ì„œ ìˆ˜
        batch_size: CrossEncoder ë°°ì¹˜ ì‚¬ì´ì¦ˆ (RTX 2070: 64 ê¶Œì¥)

    Returns:
        ë¦¬ë­í‚¹ëœ ìƒìœ„ top_kê°œ (score, Document) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
    """
    from processor import clean_json_to_text

    # (ì¿¼ë¦¬, ì •ì œëœ ë¬¸ì„œí…ìŠ¤íŠ¸) ìŒ êµ¬ì„±
    pairs = [[query, clean_json_to_text(doc.page_content)] for doc in docs]

    # GPU fp16 AMP ì ìš©í•˜ì—¬ ì˜ˆì¸¡
    with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):
        scores = reranker.predict(
            pairs,
            batch_size=batch_size,
            show_progress_bar=False,
        )

    reranked = sorted(zip(scores, docs), key=lambda x: x[0], reverse=True)
    return list(reranked[:top_k])
