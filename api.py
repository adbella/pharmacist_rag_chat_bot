"""
api.py
FastAPI ë°±ì—”ë“œ â€“ ê¸°ì¡´ RAG íŒŒì´í”„ë¼ì¸ì„ REST/SSE ì—”ë“œí¬ì¸íŠ¸ë¡œ ë…¸ì¶œí•©ë‹ˆë‹¤.

ì‹¤í–‰: python api.py
     (í˜¹ì€ uvicorn api:app --host 0.0.0.0 --port 8000 --reload)
"""

import os
import time
import json
import math
import asyncio
import logging
from typing import AsyncGenerator

try:
    from tavily import TavilyClient
    _TAVILY_AVAILABLE = True
except ImportError:
    _TAVILY_AVAILABLE = False

import torch
from fastapi import FastAPI, HTTPException, Response
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from contextlib import asynccontextmanager
from pydantic import BaseModel
from dotenv import load_dotenv

if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    torch.set_float32_matmul_precision("high")

from retriever import (
    load_embeddings,
    load_vector_db_with_embeddings,
    load_reranker,
    build_bm25_retriever,
    get_ensemble_results,
    rerank_docs,
)
from generator import (
    build_context,
    generate_answer,
    verify_answer,
    self_correction_loop,
    get_query_optimizer,
    evaluate_with_ragas,
    get_answer_prompt,
)
from processor import clear_gpu, get_gpu_status

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = asyncio.get_event_loop()
    await loop.run_in_executor(None, _load_all_resources)
    yield
    _resources.clear()


app = FastAPI(title="ì•½ì‚¬ AI ì±—ë´‡ API", version="2.0", lifespan=lifespan)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

DB_PATH = "./chroma_db_combined_1771477980"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OOS_GUARD_ENABLED = os.getenv("OOS_GUARD_ENABLED", "true").lower() in {"1", "true", "yes", "on"}
OOS_MIN_RELEVANCE = float(os.getenv("OOS_MIN_RELEVANCE", "0.55"))
OOS_MIN_TOP_SCORE = float(os.getenv("OOS_MIN_TOP_SCORE", "0.01"))
USE_QUERY_OPTIMIZER = os.getenv("USE_QUERY_OPTIMIZER", "true").lower() in {"1", "true", "yes", "on"}
VERIFY_MODEL = os.getenv("VERIFY_MODEL", "gpt-5.2")
RAGAS_MODEL = os.getenv("RAGAS_MODEL", "gpt-5.2")
RERANK_BATCH_SIZE = max(1, int(os.getenv("RERANK_BATCH_SIZE", "32")))
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY", "")
WEB_SEARCH_ENABLED = os.getenv("WEB_SEARCH_ENABLED", "true").lower() in {"1", "true", "yes", "on"}
WEB_SEARCH_MAX_RESULTS = int(os.getenv("WEB_SEARCH_MAX_RESULTS", "5"))

_resources: dict = {}
_init_done = False
_init_logs: list[str] = []


def _log_init(msg: str):
    logger.info(msg)
    _init_logs.append(f"[{time.strftime('%H:%M:%S')}] {msg}")


def _load_all_resources():
    global _resources, _init_done
    if _init_done:
        return
    _log_init("ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™” ì‹œì‘...")
    _resources["embeddings"] = load_embeddings()
    _log_init("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    _resources["vector_db"] = load_vector_db_with_embeddings(DB_PATH, _resources["embeddings"])
    _log_init("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì™„ë£Œ")
    _resources["reranker"] = load_reranker()
    _log_init("ë¦¬ë­ì»¤ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    _resources["bm25"], _resources["bm25_docs"], _resources["kiwi"] = build_bm25_retriever(vector_db=_resources["vector_db"])
    _log_init("BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    _resources["query_optimizer"] = get_query_optimizer(OPENAI_API_KEY) if USE_QUERY_OPTIMIZER else None
    _log_init("ì¿¼ë¦¬ ìµœì í™”ê¸° ì¤€ë¹„ ì™„ë£Œ" if USE_QUERY_OPTIMIZER else "ì¿¼ë¦¬ ìµœì í™”ê¸° ë¹„í™œì„±í™”(ì†ë„ ìš°ì„ )")
    _init_done = True
    _log_init("ëª¨ë“  ë¦¬ì†ŒìŠ¤ ì¤€ë¹„ ì™„ë£Œ!")


def _web_search(query: str) -> list[dict]:
    """Tavily ì›¹ ê²€ìƒ‰ â€” ì°¸ê³  ë§í¬ìš© (contextì— í¬í•¨í•˜ì§€ ì•ŠìŒ)."""
    if not _TAVILY_AVAILABLE or not TAVILY_API_KEY or not WEB_SEARCH_ENABLED:
        return []
    try:
        client = TavilyClient(api_key=TAVILY_API_KEY)
        results = client.search(
            query=f"{query} ì•½ë¬¼ ì˜ì•½í’ˆ ë³µìš©",
            search_depth="basic",
            max_results=WEB_SEARCH_MAX_RESULTS,
        )
        refs = []
        for r in results.get("results", []):
            refs.append({
                "title": r.get("title", ""),
                "url": r.get("url", ""),
                "snippet": (r.get("content", "") or "")[:150],
            })
        logger.info("ì›¹ ê²€ìƒ‰ ì™„ë£Œ: %dê±´", len(refs))
        return refs
    except Exception as e:
        logger.warning("ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: %s", e)
        return []


class ChatRequest(BaseModel):
    query: str
    model: str = "gpt-5.1"
    top_k: int = 5
    ensemble_k: int = 20
    weight_bm25: float = 0.8
    use_self_correction: bool = True
    long_answer: bool = False


@app.get("/health")
async def health():
    gpu_info = {"available": False}
    if torch.cuda.is_available():
        gpu_info = get_gpu_status()
    return {
        "status": "ready" if _init_done else "initializing",
        "init_logs": _init_logs[-5:],
        "gpu": gpu_info,
    }


@app.post("/clear-memory")
async def clear_memory():
    logger.info("GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ìš”ì²­ ìˆ˜ì‹ ")
    clear_gpu()
    return await health()


@app.get("/favicon.ico", include_in_schema=False)
async def favicon():
    return Response(status_code=204)


@app.post("/chat")
async def chat_stream(req: ChatRequest):
    if not _init_done:
        raise HTTPException(503, "ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™” ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
    if not OPENAI_API_KEY:
        raise HTTPException(400, ".envì— OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")

    async def generate() -> AsyncGenerator[str, None]:
        total_start = time.time()
        done_sent = False
        verify_task = None
        ragas_task = None
        web_search_task = None
        search_elapsed = 0.0
        rerank_elapsed = 0.0
        gen_elapsed = 0.0
        verify_elapsed = 0.0
        search_breakdown: dict[str, float] = {}
        rerank_breakdown: dict[str, float] = {}
        ensemble_docs = []
        final_docs = []
        docs_payload: list[dict] = []

        async def _cancel_pending_tasks():
            pending = [t for t in (verify_task, ragas_task) if t is not None and not t.done()]
            if pending:
                for t in pending:
                    t.cancel()
                try:
                    await asyncio.gather(*pending, return_exceptions=True)
                except Exception:
                    pass

        # NOTE: Do NOT capture the event loop here â€” always use asyncio.get_event_loop()
        # at point-of-use to avoid holding a reference to a closed/stale loop.

        def _sse(event: str, data: dict) -> str:
            return f"event: {event}\ndata: {json.dumps(data, ensure_ascii=False)}\n\n"

        import re
        def _is_pass(vr: str) -> bool:
            """ê²€ì¦ ê²°ê³¼ì—ì„œ [ìµœì¢… íŒì •] íŒŒì‹±. ë¶„ì„ ì½”ë©˜íŠ¸ì˜ PASS/FAIL í˜¼ì¬ë¥¼ ë¬´ì‹œ."""
            # [ìµœì¢… íŒì •]: PASS ë˜ëŠ” FAIL íŒ¨í„´ ìš°ì„  íƒìƒ‰
            m = re.search(r'\[ìµœì¢…\s*íŒì •\]\s*[:ï¼š]\s*(PASS|FAIL)', vr, re.IGNORECASE)
            if m:
                result = m.group(1).upper() == 'PASS'
                logger.info("[Verdict] Pattern match: '%s' -> %s", m.group(0), result)
                return result
            # íŒ¨í„´ ëª» ì°¾ìœ¼ë©´ ë§ˆì§€ë§‰ PASS/FAIL ë‹¨ì–´ ê¸°ì¤€
            tokens = re.findall(r'\b(PASS|FAIL)\b', vr, re.IGNORECASE)
            if tokens:
                result = tokens[-1].upper() == 'PASS'
                logger.info("[Verdict] Fallback last token: '%s' -> %s", tokens[-1], result)
                return result
            logger.warning("[Verdict] No PASS/FAIL found in verify_result")
            return False

        try:
            yield _sse("status", {"step": "ê²€ìƒ‰ ì¤‘...", "icon": "ğŸ”"})
            search_start = time.time()
            ensemble_docs, search_breakdown = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: get_ensemble_results(
                    query=req.query,
                    kiwi=_resources["kiwi"],
                    bm25_retriever=_resources["bm25"],
                    vector_db=_resources["vector_db"],
                    bm25_docs=_resources["bm25_docs"],
                    query_optimizer=_resources.get("query_optimizer"),
                    k=req.ensemble_k,
                    weight_bm25=req.weight_bm25,
                    weight_vector=round(1.0 - req.weight_bm25, 2),
                    return_metrics=True,
                ),
            )
            search_elapsed = time.time() - search_start

            # ë¦¬ë­í‚¹ ì¿¼ë¦¬: ì˜µí‹°ë§ˆì´ì €ê°€ ìˆìœ¼ë©´ ì˜ë£Œ í•µì‹¬ì–´ë¡œ ì¬êµ¬ì„±
            rerank_query = req.query
            if USE_QUERY_OPTIMIZER and _resources.get("query_optimizer"):
                try:
                    from langchain_core.output_parsers import StrOutputParser
                    opt = _resources["query_optimizer"]
                    clean_prompt = (
                        "ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ì•½í•™/ì˜í•™ ê´€ë ¨ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•˜ì—¬ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ë³€í™˜í•˜ì„¸ìš”.\n"
                        "ë¹„ì˜í•™ì  í‘œí˜„(ë­ë¨¹ì„ê¹Œ, ì–´ë–¡í•´, ê´œì°®ì„ê¹Œ ë“±)ì€ ì œê±°í•˜ì„¸ìš”.\n"
                        "ì¶œë ¥: í•µì‹¬ ê²€ìƒ‰ ì¿¼ë¦¬ë§Œ (í•œ ì¤„)\n"
                        f"ì…ë ¥: {req.query}"
                    )
                    rerank_query = opt.invoke(clean_prompt).strip().strip('"').strip("'")
                    logger.info("ë¦¬ë­í‚¹ ì¿¼ë¦¬ ìµœì í™”: '%s' -> '%s'", req.query, rerank_query)
                except Exception as e:
                    logger.warning("ë¦¬ë­í‚¹ ì¿¼ë¦¬ ìµœì í™” ì‹¤íŒ¨: %s", e)

            yield _sse("status", {"step": f"{len(ensemble_docs)}ê°œ ë¬¸ì„œ ë¦¬ë­í‚¹ ì¤‘...", "icon": "âš¡"})
            rerank_start = time.time()
            ranked_pairs, rerank_breakdown = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: rerank_docs(
                    query=rerank_query,
                    docs=ensemble_docs,
                    reranker=_resources["reranker"],
                    top_k=req.top_k,
                    batch_size=RERANK_BATCH_SIZE,
                    return_metrics=True,
                ),
            )
            rerank_elapsed = time.time() - rerank_start

            rerank_scores = [s for s, _ in ranked_pairs]
            final_docs = [d for _, d in ranked_pairs]

            # ìµœì†Œ ì ìˆ˜ ì„ê³„ê°’ ì ìš©: ê´€ë ¨ì„± ë‚®ì€ ë¬¸ì„œ ì œê±°
            MIN_RERANK_SCORE = 0.005
            filtered = [(s, d) for s, d in zip(rerank_scores, final_docs) if s >= MIN_RERANK_SCORE]
            if filtered:
                rerank_scores, final_docs = zip(*filtered)
                rerank_scores = list(rerank_scores)
                final_docs = list(final_docs)
            else:
                # ëª¨ë“  ë¬¸ì„œê°€ ì„ê³„ê°’ ì´í•˜ë©´ ìƒìœ„ 1ê°œë¼ë„ ìœ ì§€
                rerank_scores = rerank_scores[:1]
                final_docs = final_docs[:1]
            logger.info("ë¬¸ì„œ í•„í„°ë§: %d/%dê°œ (ì„ê³„ê°’ %.3f)", len(final_docs), len(ranked_pairs), MIN_RERANK_SCORE)

            def _build_docs_payload() -> list[dict]:
                payload = []
                local_max_score = max(rerank_scores) if rerank_scores else 1.0
                for i, (score, doc) in enumerate(zip(rerank_scores, final_docs), 1):
                    if score < MIN_RERANK_SCORE:
                        continue
                    pct = min(score / max(local_max_score, 1e-6), 1.0)
                    payload.append({
                        "rank": i,
                        "source": os.path.basename(doc.metadata.get("source", "Unknown")),
                        "score": round(float(score), 4),
                        "pct": round(float(pct) * 100, 1),
                        "preview": doc.page_content.replace("passage: ", "").replace("\n", " ")[:280],
                        "content": doc.page_content.replace("passage: ", "")[:1500],
                    })
                return payload

            docs_payload = _build_docs_payload()
            yield _sse("docs", {"docs": docs_payload})

            top_score = float(rerank_scores[0]) if rerank_scores else -999.0
            top_relevance = 1.0 / (1.0 + math.exp(-max(min(top_score, 30.0), -30.0)))
            should_oos_block = (
                OOS_GUARD_ENABLED
                and (top_relevance < OOS_MIN_RELEVANCE)
                and (top_score < OOS_MIN_TOP_SCORE)
            )

            if should_oos_block:
                oos_answer = (
                    "ì œê³µëœ ë¬¸ì„œì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. "
                    "í˜„ì¬ ë³´ìœ í•œ ê·¼ê±° ë²”ìœ„ ë°– ì§ˆë¬¸ìœ¼ë¡œ íŒë‹¨ë˜ì–´ ì¶”ì¸¡ ë‹µë³€ì„ ìƒëµí•©ë‹ˆë‹¤. "
                    "ê´€ë ¨ ì˜ì•½í’ˆ/ì¦ìƒ í‚¤ì›Œë“œë¥¼ ë” êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ë‹¤ì‹œ í™•ì¸í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
                )
                total_elapsed_oos = time.time() - total_start
                yield _sse("done", {
                    "answer": oos_answer,
                    "is_pass": True,
                    "correction_rounds": 0,
                    "correction_logs": [],
                    "verify_result": (
                        f"OOS_GUARD (top_relevance={top_relevance:.3f}, rel_threshold={OOS_MIN_RELEVANCE:.3f}, "
                        f"top_score={top_score:.4f}, score_threshold={OOS_MIN_TOP_SCORE:.4f})"
                    ),
                    "metrics_pending": False,
                    "ragas": {"faithfulness": 0.0, "answer_relevancy": 0.0},
                    "docs": docs_payload,
                    "metrics": {
                        "search_s": round(search_elapsed, 3),
                        "rerank_s": round(rerank_elapsed, 3),
                        "gen_s": 0.0,
                        "verify_s": 0.0,
                        "total_s": round(total_elapsed_oos, 3),
                        "ensemble_n": len(ensemble_docs),
                        "final_n": len(final_docs),
                        "top_score": round(top_score, 4),
                        "top_relevance": round(top_relevance, 4),
                        "oos_min_top_score": round(OOS_MIN_TOP_SCORE, 4),
                        "oos_guard": True,
                        **{k: round(float(v), 3) for k, v in search_breakdown.items()},
                        **{k: round(float(v), 3) for k, v in rerank_breakdown.items()},
                    },
                })
                return

            context_text = build_context(final_docs)

            # ì›¹ ê²€ìƒ‰ì„ ë‹µë³€ ìƒì„±ê³¼ ë³‘ë ¬ë¡œ ì‹œì‘ (contextì— í¬í•¨í•˜ì§€ ì•ŠìŒ, ë§í¬ë§Œ ì œê³µ)
            async def _run_web_search():
                return await asyncio.get_event_loop().run_in_executor(
                    None, lambda: _web_search(req.query)
                )
            if WEB_SEARCH_ENABLED and TAVILY_API_KEY:
                web_search_task = asyncio.create_task(_run_web_search())

            yield _sse("status", {"step": "ë‹µë³€ ìƒì„± ì¤‘...", "icon": "âœï¸"})
            gen_start = time.time()

            initial_answer = ""
            answer_prompt = get_answer_prompt(req.long_answer)
            async_stream = await generate_answer(
                query=req.query,
                context_text=context_text,
                openai_api_key=OPENAI_API_KEY,
                model=req.model,
                prompt_template_str=answer_prompt,
                stream=True,
                async_mode=True,
            )

            async for chunk in async_stream:
                if chunk:
                    initial_answer += chunk
                    yield _sse("token", {"text": chunk})

            gen_elapsed = time.time() - gen_start
            yield _sse("status", {"step": "í’ˆì§ˆ ê²€ì¦ ë° ì§€í‘œ ë¶„ì„ ì¤‘...", "icon": "âš¡"})

            async def _run_verify():
                return await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: verify_answer(
                        query=req.query,
                        context_text=context_text,
                        answer=initial_answer,
                        openai_api_key=OPENAI_API_KEY,
                        model=VERIFY_MODEL,
                    )
                )

            async def _run_ragas():
                if req.model == "debug":
                    return {"faithfulness": 0.0, "answer_relevancy": 0.0}
                return await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: evaluate_with_ragas(
                        query=req.query,
                        answer=initial_answer,
                        final_docs=final_docs,
                        embeddings=_resources["embeddings"],
                        openai_api_key=OPENAI_API_KEY,
                        eval_model=RAGAS_MODEL,
                    )
                )

            verify_task = asyncio.create_task(_run_verify())
            ragas_task = asyncio.create_task(_run_ragas())

            verify_result = await verify_task
            verify_elapsed = time.time() - gen_start - gen_elapsed

            final_answer = initial_answer
            correction_rounds = 0
            correction_logs: list[dict] = []

            is_pass = _is_pass(verify_result)
            yield _sse("verdict", {
                "is_pass": is_pass,
                "verify_result": verify_result
            })

            if req.use_self_correction and not is_pass:
                yield _sse("status", {"step": "ìë™ í”„ë¡¬í”„íŠ¸ ìµœì í™” ì‹œì‘...", "icon": "ğŸ¤–"})
                if ragas_task and not ragas_task.done():
                    ragas_task.cancel()

                async for event_type, value in self_correction_loop(
                    query=req.query,
                    context_text=context_text,
                    initial_answer=initial_answer,
                    initial_verify_result=verify_result,
                    openai_api_key=OPENAI_API_KEY,
                    gen_model=req.model,
                    max_rounds=3,
                    initial_ragas_result=None,
                    embeddings=_resources["embeddings"],
                    final_docs=final_docs,
                ):
                    if event_type == "status":
                        yield _sse("status", value)
                    elif event_type == "token":
                        yield _sse("token", {"text": value})
                    elif event_type == "done_loop":
                        final_answer = value["answer"]
                        verify_result = value["verify_result"]
                        correction_rounds = value["rounds"]
                        correction_logs = value["logs"]

                yield _sse("status", {"step": "êµì • ì™„ë£Œ!", "icon": "âœ…"})

            # â”€â”€ êµì • ì‹œ RAGAS ì¬ì‹¤í–‰ (êµì •ëœ ë‹µë³€ ê¸°ì¤€) â”€â”€
            if correction_rounds > 0:
                # êµì •ì´ ë°œìƒí–ˆìœ¼ë©´ ê¸°ì¡´ ragas_task ì·¨ì†Œ, êµì •ëœ ë‹µë³€ìœ¼ë¡œ ì¬í‰ê°€
                if ragas_task and not ragas_task.done():
                    ragas_task.cancel()

                async def _run_ragas_corrected():
                    return await asyncio.get_event_loop().run_in_executor(
                        None,
                        lambda: evaluate_with_ragas(
                            query=req.query,
                            answer=final_answer,
                            final_docs=final_docs,
                            embeddings=_resources["embeddings"],
                            openai_api_key=OPENAI_API_KEY,
                            eval_model=RAGAS_MODEL,
                        )
                    )
                ragas_task = asyncio.create_task(_run_ragas_corrected())

            # â”€â”€ RAGAS ê²°ê³¼ ëŒ€ê¸° (ë™ê¸° â€” heartbeatë¡œ ì—°ê²° ìœ ì§€) â”€â”€
            yield _sse("status", {"step": "RAGAS ì„±ëŠ¥ í‰ê°€ ì¤‘...", "icon": "ğŸ“Š", "progress": 0})
            ragas_results = {"faithfulness": 0.0, "answer_relevancy": 0.0}
            ragas_start = time.time()
            RAGAS_EXPECTED_SECS = 80.0  # ì˜ˆìƒ ì†Œìš” ì‹œê°„
            if ragas_task and not ragas_task.cancelled():
                try:
                    while not ragas_task.done():
                        await asyncio.sleep(5.0)
                        ragas_elapsed = time.time() - ragas_start
                        total_elapsed = time.time() - total_start
                        pct = min(int((ragas_elapsed / RAGAS_EXPECTED_SECS) * 95), 95)
                        yield _sse("status", {
                            "step": f"RAGAS í‰ê°€ ì§„í–‰ ì¤‘... ({total_elapsed:.0f}ì´ˆ)",
                            "icon": "ğŸ“Š",
                            "progress": pct,
                        })
                    ragas_results = ragas_task.result()
                    logger.info("RAGAS ë™ê¸° í‰ê°€ ì™„ë£Œ: %s", ragas_results)
                    yield _sse("status", {"step": "RAGAS í‰ê°€ ì™„ë£Œ!", "icon": "âœ…", "progress": 100})
                except asyncio.CancelledError:
                    logger.info("RAGAS task cancelled (client disconnected)")
                    raise
                except Exception as ragas_err:
                    logger.warning("RAGAS evaluation failed: %s", ragas_err)

            # â”€â”€ done ì´ë²¤íŠ¸ ì „ì†¡ (RAGAS ì ìˆ˜ í¬í•¨) â”€â”€
            total_elapsed_done = time.time() - total_start
            done_sent = True
            yield _sse("done", {
                "answer": final_answer,
                "is_pass": _is_pass(verify_result),
                "correction_rounds": correction_rounds,
                "correction_logs": correction_logs,
                "verify_result": verify_result,
                "metrics_pending": False,
                "ragas": ragas_results,
                "docs": docs_payload,
                "metrics": {
                    "search_s": round(search_elapsed, 3),
                    "rerank_s": round(rerank_elapsed, 3),
                    "gen_s": round(gen_elapsed, 3),
                    "verify_s": round(verify_elapsed, 3),
                    "total_s": round(total_elapsed_done, 3),
                    "ensemble_n": len(ensemble_docs),
                    "final_n": len(final_docs),
                    **{k: round(float(v), 3) for k, v in search_breakdown.items()},
                    **{k: round(float(v), 3) for k, v in rerank_breakdown.items()},
                },
            })

            # â”€â”€ ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì‹  (ì°¸ê³  ë§í¬ìš©, contextì— ë¯¸í¬í•¨) â”€â”€
            if web_search_task and not web_search_task.cancelled():
                try:
                    web_refs = await asyncio.wait_for(web_search_task, timeout=5.0)
                    if web_refs:
                        yield _sse("web_refs", {"refs": web_refs})
                except asyncio.TimeoutError:
                    logger.warning("ì›¹ ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ (5ì´ˆ)")
                except Exception as web_err:
                    logger.warning("ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì‹  ì‹¤íŒ¨: %s", web_err)

        except asyncio.CancelledError:
            logger.info("Chat stream cancelled by client.")
            # _cancel_pending_tasks()ëŠ” ì—¬ê¸°ì„œ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ â€” finallyì—ì„œ ì²˜ë¦¬
            # ragas_taskëŠ” backgroundì—ì„œ ê³„ì† ì‹¤í–‰ë˜ë¯€ë¡œ cancelí•˜ì§€ ì•ŠìŒ
            if verify_task and not verify_task.done():
                verify_task.cancel()
            return
        except GeneratorExit:
            logger.info("Chat generator exited (client closed connection).")
            if verify_task and not verify_task.done():
                verify_task.cancel()
            return
        except Exception as e:
            logger.exception("Chat error: %s", e)
            total_elapsed_error = time.time() - total_start
            try:
                if done_sent:
                    yield _sse("error", {"message": f"í›„ì† ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"})
                else:
                    yield _sse("done", {
                        "answer": "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                        "is_pass": False,
                        "correction_rounds": 0,
                        "correction_logs": [],
                        "verify_result": f"ERROR: {str(e)}",
                        "metrics_pending": False,
                        "ragas": {"faithfulness": 0.0, "answer_relevancy": 0.0},
                        "docs": docs_payload,
                        "metrics": {
                            "search_s": round(search_elapsed, 3),
                            "rerank_s": round(rerank_elapsed, 3),
                            "gen_s": round(gen_elapsed, 3),
                            "verify_s": round(verify_elapsed, 3),
                            "total_s": round(total_elapsed_error, 3),
                            "ensemble_n": len(ensemble_docs),
                            "final_n": len(final_docs),
                            "error": True,
                            **{k: round(float(v), 3) for k, v in search_breakdown.items()},
                            **{k: round(float(v), 3) for k, v in rerank_breakdown.items()},
                        },
                    })
            except (GeneratorExit, asyncio.CancelledError):
                pass  # í´ë¼ì´ì–¸íŠ¸ê°€ ì´ë¯¸ ëŠê¸´ ê²½ìš° â€” ì •ìƒ
            except Exception:
                pass
        finally:
            # verify_taskë§Œ ì •ë¦¬ (ragas_taskëŠ” background ì™„ë£Œ í—ˆìš©)
            if verify_task and not verify_task.done():
                verify_task.cancel()
                try:
                    await asyncio.shield(asyncio.gather(verify_task, return_exceptions=True))
                except Exception:
                    pass

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",
        },
    )


STATIC_DIR = os.path.join(os.path.dirname(__file__), "web-ui")
if os.path.exists(STATIC_DIR):
    app.mount("/static", StaticFiles(directory=STATIC_DIR), name="static")

    @app.get("/")
    async def serve_index():
        return FileResponse(os.path.join(STATIC_DIR, "index.html"))


if __name__ == "__main__":
    import uvicorn
    uvicorn.run("api:app", host="0.0.0.0", port=8000, reload=False)
