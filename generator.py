"""
generator.py
GPT ë‹µë³€ ìƒì„±, ìžê¸° ê²€ì¦(Verifier), RAGAS í‰ê°€ í•¨ìˆ˜.
"""

import os
import json
import time
import asyncio
import logging
from datasets import Dataset

from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy

from processor import clean_json_to_text, get_clean_doc_text


logger = logging.getLogger(__name__)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ì „ì—­ ìƒìˆ˜)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_ANSWER_PROMPT_TEMPLATE = """\
ë‹¹ì‹ ì€ ê³µì¸ëœ ì „ë¬¸ ì•½ì‚¬ìž…ë‹ˆë‹¤.
ì œê³µëœ [ê²€ìƒ‰ëœ ë¬¸ì„œ]ì— ê·¼ê±°í•˜ì—¬ ë‹µë³€í•˜ì‹­ì‹œì˜¤.

âš ï¸ ì§€ì¹¨:
1. ëª¨ë“  ë‹µë³€ì€ [ê²€ìƒ‰ëœ ë¬¸ì„œ]ì— ê¸°ìž¬ëœ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤. ë¬¸ì„œì— ì—†ëŠ” ì„±ë¶„ëª…, ìš©ëŸ‰, ì§ˆí™˜ëª…, ìƒí˜¸ìž‘ìš© ì •ë³´ë¥¼ ì§ì ‘ ì¶”ê°€í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
2. **ì ˆëŒ€ë¡œ ë‹¹ì‹ ì˜ ì‚¬ì „ í•™ìŠµ ì§€ì‹, ì™¸ë¶€ ì›¹ì‚¬ì´íŠ¸ ì •ë³´, ë˜ëŠ” ì¼ë°˜ ìƒì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.** ì˜¤ì§ ì•„ëž˜ [ê²€ìƒ‰ëœ ë¬¸ì„œ]ì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ë‹µë³€ì— í¬í•¨í•˜ì‹­ì‹œì˜¤.
3. ë¬¸ì„œì— ë¶€ìž‘ìš©ì˜ ê°•ë„ê°€ ì „í˜€ ì–¸ê¸‰ë˜ì§€ ì•Šì•˜ë‹¤ë©´, ìž„ì˜ë¡œ ë“±ê¸‰ì„ ë§¤ê¸°ì§€ ë§ˆì‹­ì‹œì˜¤.
4. ì¶œì²˜ í‘œê¸°ëŠ” **ë‹¨ë½ ëì´ë‚˜ í•µì‹¬ ì •ë³´ ë’¤ì— í•œ ë²ˆë§Œ** [ë¬¸ì„œ N] í˜•ì‹ìœ¼ë¡œ í‘œê¸°í•˜ì‹­ì‹œì˜¤. ë§¤ ë¬¸ìž¥ë§ˆë‹¤ ë°˜ë³µí•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
5. ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µì´ ë¬¸ì„œì— ì—†ë”ë¼ë„, ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©ì´ ìžˆìœ¼ë©´ í•´ë‹¹ ë¬¸ì„œ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ì¸ìš©í•˜ì—¬ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
6. ë‹µë³€ì˜ ì²« ë¬¸ìž¥ì—ì„œ ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œ(ì•½í’ˆëª…, ì¦ìƒëª… ë“±)ë¥¼ í¬í•¨í•˜ì—¬ ì§ˆë¬¸ì— ì§ì ‘ ë‹µí•˜ì‹­ì‹œì˜¤.
7. **10ë¬¸ìž¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ê³  ë¶ˆí•„ìš”í•œ ë°˜ë³µì„ í”¼í•˜ì„¸ìš”**
8. ëª¨ë“  ë‹µë³€ì´ ëë‚œ í›„ì— "ìžì„¸í•œ ë‚´ìš©ì€ ì „ë¬¸ê°€ì™€ ê¼­ ìƒë‹´í•˜ì„¸ìš”."ë¼ëŠ” ë¬¸êµ¬ë¥¼ í¬í•¨í•˜ì‹­ì‹œì˜¤.


[ê²€ìƒ‰ëœ ë¬¸ì„œ]
{context}

[ì§ˆë¬¸]
{question}

[ë‹µë³€]"""

_ANSWER_PROMPT_LONG = """\
ë‹¹ì‹ ì€ ê³µì¸ëœ ì „ë¬¸ ì•½ì‚¬ìž…ë‹ˆë‹¤.
ì œê³µëœ [ê²€ìƒ‰ëœ ë¬¸ì„œ]ì— ê·¼ê±°í•˜ì—¬ ë‹µë³€í•˜ì‹­ì‹œì˜¤.

âš ï¸ ì§€ì¹¨:
1. ëª¨ë“  ë‹µë³€ì€ [ê²€ìƒ‰ëœ ë¬¸ì„œ]ì— ê¸°ìž¬ëœ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤. ë¬¸ì„œì— ì—†ëŠ” ì„±ë¶„ëª…, ìš©ëŸ‰, ì§ˆí™˜ëª…, ìƒí˜¸ìž‘ìš© ì •ë³´ë¥¼ ì§ì ‘ ì¶”ê°€í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
2. **ì ˆëŒ€ë¡œ ë‹¹ì‹ ì˜ ì‚¬ì „ í•™ìŠµ ì§€ì‹, ì™¸ë¶€ ì›¹ì‚¬ì´íŠ¸ ì •ë³´, ë˜ëŠ” ì¼ë°˜ ìƒì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.** ì˜¤ì§ ì•„ëž˜ [ê²€ìƒ‰ëœ ë¬¸ì„œ]ì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ë‹µë³€ì— í¬í•¨í•˜ì‹­ì‹œì˜¤.
3. ë¬¸ì„œì— ë¶€ìž‘ìš©ì˜ ê°•ë„ê°€ ì „í˜€ ì–¸ê¸‰ë˜ì§€ ì•Šì•˜ë‹¤ë©´, ìž„ì˜ë¡œ ë“±ê¸‰ì„ ë§¤ê¸°ì§€ ë§ˆì‹­ì‹œì˜¤.
4. ì¶œì²˜ í‘œê¸°ëŠ” **ë‹¨ë½ ëì´ë‚˜ í•µì‹¬ ì •ë³´ ë’¤ì— í•œ ë²ˆë§Œ** [ë¬¸ì„œ N] í˜•ì‹ìœ¼ë¡œ í‘œê¸°í•˜ì‹­ì‹œì˜¤. ë§¤ ë¬¸ìž¥ë§ˆë‹¤ ë°˜ë³µí•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
5. ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µì´ ë¬¸ì„œì— ì—†ë”ë¼ë„, ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©ì´ ìžˆìœ¼ë©´ í•´ë‹¹ ë¬¸ì„œ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ì¸ìš©í•˜ì—¬ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
6. ë‹µë³€ì˜ ì²« ë¬¸ìž¥ì—ì„œ ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œ(ì•½í’ˆëª…, ì¦ìƒëª… ë“±)ë¥¼ í¬í•¨í•˜ì—¬ ì§ˆë¬¸ì— ì§ì ‘ ë‹µí•˜ì‹­ì‹œì˜¤.
7. ëª¨ë“  ë‹µë³€ì´ ëë‚œ í›„ì— "ìžì„¸í•œ ë‚´ìš©ì€ ì „ë¬¸ê°€ì™€ ê¼­ ìƒë‹´í•˜ì„¸ìš”."ë¼ëŠ” ë¬¸êµ¬ë¥¼ í¬í•¨í•˜ì‹­ì‹œì˜¤.

[ê²€ìƒ‰ëœ ë¬¸ì„œ]
{context}

[ì§ˆë¬¸]
{question}

[ë‹µë³€]"""


def get_answer_prompt(long_answer: bool = False) -> str:
    """long_answer ëª¨ë“œì— ë”°ë¼ ì ì ˆí•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return _ANSWER_PROMPT_LONG if long_answer else _ANSWER_PROMPT_TEMPLATE

_VERIFY_PROMPT_TEMPLATE = """\
ë‹¹ì‹ ì€ 'ì‹í’ˆì˜ì•½í’ˆ ì•ˆì „ì²˜', ë¯¸êµ­ 'FDA' ë“± ê³µì‹ ë ¥ ìžˆëŠ” í—ˆê°€ ê¸°ê´€ì˜ ì—„ê²©í•œ ê°ë…ê´€ìž…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì—­í• ì€ ì „ë¬¸ì•½ì‚¬ê°€ ìž‘ì„±í•œ [ê²€ì¦ ëŒ€ìƒ ë‹µë³€]ì´ [ê²€ìƒ‰ëœ ë¬¸ì„œ]ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **ë…¼ë¦¬ì ìœ¼ë¡œ íƒ€ë‹¹í•œì§€** í‰ê°€í•˜ëŠ” ê²ƒìž…ë‹ˆë‹¤.

[ê²€ìƒ‰ëœ ë¬¸ì„œ (Ground Truth)]
{context}

[ì§ˆë¬¸]
{question}

[ê²€ì¦ ëŒ€ìƒ ë‹µë³€]
{answer}

[í‰ê°€ ê¸°ì¤€]
1. ë…¼ë¦¬ì  ë¹„ì•½: ë¬¸ì„œì— ì§ì ‘ì ì¸ ë‹¨ì–´ê°€ ì—†ë”ë¼ë„, ë¬¸ì„œ ë‚´ìš©ìœ¼ë¡œë¶€í„° í•©ë¦¬ì ìœ¼ë¡œ ìœ ì¶”í•œ ê²ƒì´ë¼ë©´ PASSë¡œ íŒì •í•˜ì‹­ì‹œì˜¤.
2. í™˜ê°(Hallucination): ë¬¸ì„œì— ì „í˜€ ì—†ëŠ” ë‚´ìš©ì„ ê·¼ê±° ì—†ì´ ì§€ì–´ëƒˆì„ ë•Œë§Œ FAILë¡œ íŒì •í•˜ì‹­ì‹œì˜¤.
3. ì•ˆì „ ê¶Œê³ : "ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì„¸ìš”" ë“± í™˜ìž ì•ˆì „ì„ ìœ„í•œ ê¸°ë³¸ ê¶Œê³ ëŠ” PASSë¡œ í—ˆìš©í•©ë‹ˆë‹¤.

[ì¶œë ¥ í˜•ì‹]
ë°˜ë“œì‹œ ì•„ëž˜ í˜•ì‹ì„ ì§€ì¼œì£¼ì„¸ìš”.
- [ë¶„ì„ ì½”ë©˜íŠ¸]: (ê·¼ê±°ì™€ ì£¼ìž¥ì˜ ì—°ê²°ê³ ë¦¬ê°€ íƒ€ë‹¹í•œì§€ ì„¤ëª…)
- [ìµœì¢… íŒì •]: PASS ë˜ëŠ” FAIL"""

_CORRECTION_PROMPT_TEMPLATE = """\
ë‹¹ì‹ ì€ ê²€ì¦ í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìˆ˜ì •í•˜ëŠ” **ì „ë¬¸ ì•½ì‚¬**ìž…ë‹ˆë‹¤.

[ì‚¬ìš©ìž ì§ˆë¬¸]: {question}
[ê²€ìƒ‰ëœ ë¬¸ì„œ]: {context}
[ì´ì „ ë‹µë³€]: {answer}
[ê²€ì¦ í”¼ë“œë°±]: {verify_result}

ìœ„ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ì˜¤ë¥˜ë¥¼ ë°”ë¡œìž¡ê³ , ë‹¤ì‹œ ìµœì„ ì˜ ë‹µë³€ì„ ìž‘ì„±í•˜ì‹­ì‹œì˜¤.
ì£¼ì˜ì‚¬í•­:
- [ê²€ìƒ‰ëœ ë¬¸ì„œ]ì— ê´€ë ¨ ì •ë³´ê°€ ìžˆë‹¤ë©´ í•´ë‹¹ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
- ëª¨ë“  ì •ë³´ ë’¤ì— [ë¬¸ì„œ N] ì¶œì²˜ë¥¼ í‘œê¸°í•˜ì‹­ì‹œì˜¤.
- "ìžì„¸í•œ ë‚´ìš©ì€ ì „ë¬¸ê°€ì™€ ê¼­ ìƒë‹´í•˜ì„¸ìš”."ë¥¼ í¬í•¨í•˜ì‹­ì‹œì˜¤.
- ìˆ˜ì •ëœ ë‹µë³€ë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤."""

_OPTIMIZER_PROMPT_TEMPLATE = """\
ë‹¹ì‹ ì€ 'RAG ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì „ë¬¸ê°€'ìž…ë‹ˆë‹¤.
ì´ì „ ë¼ìš´ë“œì—ì„œ ìƒì„±ëœ ë‹µë³€ì´ ê²€ì¦ ì‹¤íŒ¨ íŒì •ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.

[ì‚¬ìš©ìž ì§ˆë¬¸]: {question}
[ê²€ì¦ ê²°ê³¼]: {verify_result}
[RAGAS ì§€í‘œ]: {ragas_result}
[ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿]: {original_template}

ìœ„ì˜ ì‹¤íŒ¨ ì›ì¸ê³¼ ì§€í‘œë¥¼ ë¶„ì„í•˜ì—¬, ë‹¤ìŒ ë¼ìš´ë“œì—ì„œ ë” ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ìžˆë„ë¡ ìˆ˜ì •ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ë§Œë“œì‹­ì‹œì˜¤.
- [ê²€ìƒ‰ëœ ë¬¸ì„œ]ì˜ ë°ì´í„°ë¥¼ ë” ì •í™•í•˜ê²Œ ì¸ìš©í•˜ê³  ì¶”ì¸¡ì„ ë°°ì œí•˜ë„ë¡ ì§€ì‹œë¥¼ ê°•í™”í•˜ì„¸ìš”.
- í•„ìš”í•˜ë‹¤ë©´ ì¶œë ¥ í˜•ì‹ì´ë‚˜ ì£¼ì˜ì‚¬í•­ì„ êµ¬ì²´ì ìœ¼ë¡œ ì¡°ì •í•˜ì„¸ìš”.
- ë°˜ë“œì‹œ {{{{context}}}}ì™€ {{{{question}}}} ë³€ìˆ˜ë¥¼ í¬í•¨í•œ ì „ì²´ í”„ë¡¬í”„íŠ¸ ì „ë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”."""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_context(final_docs: list[Document], max_chars: int = 1000) -> str:
    """
    ìµœì¢… ì„ íƒ ë¬¸ì„œë“¤ì„ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìžì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Args:
        final_docs: ë¦¬ëž­í‚¹ëœ Document ë¦¬ìŠ¤íŠ¸
        max_chars:  ë¬¸ì„œ ë‹¹ ìµœëŒ€ ë¬¸ìž ìˆ˜ (ê¸°ë³¸ 1500 â€“ ì •í™•ë„ í–¥ìƒ)

    Returns:
        í¬ë§·ëœ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìžì—´
    """
    parts = []
    for i, doc in enumerate(final_docs, 1):
        source = os.path.basename(doc.metadata.get("source", "Unknown"))
        content = get_clean_doc_text(doc)
        parts.append(f"[ë¬¸ì„œ {i}] (ì¶œì²˜: {source})\n{content[:max_chars]}")
    return "\n\n".join(parts)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‹µë³€ ìƒì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _get_llm(
    model: str,
    temperature: float,
    api_key: str,
    streaming: bool = False,
) -> ChatOpenAI:
    """
    LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìš”ì²­ ë‹¨ìœ„ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

    ì „ì—­ ìºì‹œ ìž¬ì‚¬ìš© ì‹œ ê°„í—ì ìœ¼ë¡œ "Event loop is closed"ê°€ ì „íŒŒë˜ëŠ” ì‚¬ë¡€ê°€ ìžˆì–´,
    ë£¨í”„/ìš”ì²­ ê²½ê³„ ê°„ ê°ì²´ ê³µìœ ë¥¼ í”¼í•˜ë„ë¡ ì•ˆì „ ëª¨ë“œë¡œ ìš´ìš©í•©ë‹ˆë‹¤.
    """
    return ChatOpenAI(
        model=model,
        temperature=temperature,
        streaming=streaming,
        api_key=api_key,
    )



def _is_rate_limit_error(exc: Exception) -> bool:
    normalized = str(exc).lower()
    return any(keyword in normalized for keyword in ("429", "rate limit", "quota"))


def _is_event_loop_closed_error(exc: Exception) -> bool:
    return "event loop is closed" in str(exc).lower()


def _retry_api_call(callable_obj, payload):
    max_attempts = 5
    backoff = 1.0
    for attempt in range(1, max_attempts + 1):
        try:
            return callable_obj(payload)
        except Exception as exc:
            if _is_event_loop_closed_error(exc) and attempt < max_attempts:
                logger.warning(
                    "Detected closed event loop. Rebuilding request chain and retrying... (%d/%d)",
                    attempt,
                    max_attempts,
                )
                time.sleep(0.2)
                continue
            if not _is_rate_limit_error(exc) or attempt == max_attempts:
                logger.error(f"API Call failed (Attempt {attempt}/{max_attempts}): {exc}")
                raise
            logger.warning(f"Rate limit hit. Retrying in {backoff}s... (Attempt {attempt}/{max_attempts})")
            time.sleep(backoff)
            backoff *= 2


async def _async_retry_api_call(callable_obj, payload):
    max_attempts = 5
    backoff = 1.0
    for attempt in range(1, max_attempts + 1):
        try:
            if asyncio.iscoroutinefunction(callable_obj):
                return await callable_obj(payload)
            else:
                # callable_obj might be a method like chain.ainvoke
                res = callable_obj(payload)
                if hasattr(res, "__await__"):
                    return await res
                return res
        except Exception as exc:
            if _is_event_loop_closed_error(exc) and attempt < max_attempts:
                logger.warning(
                    "Detected closed event loop in async call. Retrying... (%d/%d)",
                    attempt,
                    max_attempts,
                )
                await asyncio.sleep(0.2)
                continue
            if not _is_rate_limit_error(exc) or attempt == max_attempts:
                logger.error(f"Async API Call failed (Attempt {attempt}/{max_attempts}): {exc}")
                raise
            logger.warning(f"Async Rate limit hit. Retrying in {backoff}s... (Attempt {attempt}/{max_attempts})")
            await asyncio.sleep(backoff)
            backoff *= 2


async def generate_answer(
    query: str,
    context_text: str,
    openai_api_key: str,
    model: str = "gpt-5.1",
    temperature: float = 0.1,
    prompt_template_str: str = _ANSWER_PROMPT_TEMPLATE,
    stream: bool = False,
    async_mode: bool = False,
):
    """
    GPT ëª¨ë¸ë¡œ ìµœì¢… ì•½ì‚¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        stream: Trueì´ë©´ ì œë„ˆë ˆì´í„°(ì²­í¬ ì´í„°ë ˆì´í„°)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        async_mode: Trueì´ê³  stream=Trueì´ë©´ ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°(astream)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Returns:
        stream=False â†’ ì™„ì„±ëœ ë‹µë³€ ë¬¸ìžì—´
        stream=True, async_mode=False â†’ ë¬¸ìžì—´ ì²­í¬ ì´í„°ë ˆì´í„°
        stream=True, async_mode=True â†’ ë¹„ë™ê¸° ë¬¸ìžì—´ ì²­í¬ ì´í„°ë ˆì´í„°
    """
    os.environ["OPENAI_API_KEY"] = openai_api_key

    prompt = PromptTemplate.from_template(prompt_template_str)
    llm = _get_llm(
        model=model,
        temperature=temperature,
        api_key=openai_api_key,
        streaming=bool(stream or async_mode),
    )
    chain = prompt | llm | StrOutputParser()

    call = (
        chain.astream if stream and async_mode else
        chain.stream if stream else
        chain.invoke
    )
    
    if async_mode:
        if stream:
            # Note: For stream=True, this only retries the initial connection/iterator creation.
            return await _async_retry_api_call(chain.astream, {"context": context_text, "question": query})
        else:
            return await _async_retry_api_call(chain.ainvoke, {"context": context_text, "question": query})
    
    return _retry_api_call(call, {"context": context_text, "question": query})


def get_query_optimizer(openai_api_key: str, model: str = "gpt-5.2"):
    """
    ì¿¼ë¦¬ í™•ìž¥(Query Expansion)ìš© ê²½ëŸ‰ GPT ì²´ì¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì•ˆì •ì„±ì„ ìœ„í•´ í˜¸ì¶œ ì‹œ ìƒˆë¡œìš´ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    os.environ["OPENAI_API_KEY"] = openai_api_key
    return ChatOpenAI(model=model, temperature=0, api_key=openai_api_key) | StrOutputParser()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìžê¸° ê²€ì¦ (Verifier)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def verify_answer(
    query: str,
    context_text: str,
    answer: str,
    openai_api_key: str,
    model: str = "gpt-5.2",
) -> str:
    """
    GPT ê²€ì¦ê´€ì´ ë‹µë³€ì˜ ë…¼ë¦¬ì  íƒ€ë‹¹ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤. (ì‚¬ìš©ìž ìš”ì²­: gpt-5.2 ì‚¬ìš©)

    Returns:
        ê²€ì¦ ê²°ê³¼ ë¬¸ìžì—´ (PASS / FAIL í¬í•¨)
    """
    os.environ["OPENAI_API_KEY"] = openai_api_key

    prompt = PromptTemplate.from_template(_VERIFY_PROMPT_TEMPLATE)
    verifier_llm = _get_llm(
        model=model,
        temperature=0.0,
        api_key=openai_api_key,
        streaming=False,
    )
    chain = prompt | verifier_llm | StrOutputParser()

    return _retry_api_call(chain.invoke, {
        "context": context_text,
        "question": query,
        "answer": answer,
    })


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìžê¸° êµì • ë£¨í”„ (Self-Correction Loop)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def self_correction_loop(
    query: str,
    context_text: str,
    initial_answer: str,
    initial_verify_result: str,
    openai_api_key: str,
    gen_model: str = "gpt-5.1",
    max_rounds: int = 3,
    initial_ragas_result: dict = None,
    embeddings = None,
    final_docs = None,
):
    """
    FAIL íŒì • ì‹œ ê²€ì¦ ê²°ê³¼ ë° RAGAS ì§€í‘œë¥¼ ë¶„ì„í•˜ì—¬ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ìžë™ ìµœì í™”í•˜ê³  ìž¬ìƒì„±í•©ë‹ˆë‹¤.
    (ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°ë¡œ ì „í™˜í•˜ì—¬ í† í° ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
    """
    os.environ["OPENAI_API_KEY"] = openai_api_key

    current_answer = initial_answer
    last_answer_for_correction = initial_answer
    last_verify_result = initial_verify_result
    last_ragas_result = initial_ragas_result or {"faithfulness": 0.0, "answer_relevancy": 0.0}
    current_template = _ANSWER_PROMPT_TEMPLATE

    # Round 0 = ìµœì´ˆ ì‹œë„ ë¡œê·¸
    correction_logs: list[dict] = [
        {
            "round": 0,
            "answer": initial_answer,
            "verify_result": initial_verify_result,
            "ragas_result": last_ragas_result,
            "prompt_template": current_template,
        }
    ]

    for round_num in range(1, max_rounds + 1):
        # _is_pass í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•í•˜ê²Œ íŒì •
        import re
        def _is_pass_check(vr: str) -> bool:
            m = re.search(r'\[ìµœì¢…\s*íŒì •\]\s*[:ï¼š]\s*(PASS|FAIL)', vr, re.IGNORECASE)
            if m:
                return m.group(1).upper() == 'PASS'
            tokens = re.findall(r'\b(PASS|FAIL)\b', vr, re.IGNORECASE)
            return tokens[-1].upper() == 'PASS' if tokens else False
        
        if _is_pass_check(last_verify_result):
            break

        # 1. í”„ë¡¬í”„íŠ¸ ìµœì í™” (GPT-5.2 ì‚¬ìš©)
        optimizer_llm = _get_llm(
            model="gpt-4o-mini",
            temperature=0.2,
            api_key=openai_api_key,
            streaming=False,
        )
        optimizer_prompt = PromptTemplate.from_template(_OPTIMIZER_PROMPT_TEMPLATE)
        optimizer_chain = optimizer_prompt | optimizer_llm | StrOutputParser()

        yield ("status", {"step": f"í”„ë¡¬í”„íŠ¸ ìµœì í™” ì¤‘ (Round {round_num})...", "icon": "âš™ï¸"})
        
        # OptimizerëŠ” í…ìŠ¤íŠ¸ë¥¼ ë°”ë¡œ ë°˜í™˜í•˜ë¯€ë¡œ invoke ì‚¬ìš©
        new_template = await _async_retry_api_call(optimizer_chain.ainvoke, {
            "question": query,
            "verify_result": last_verify_result,
            "ragas_result": json.dumps(last_ragas_result, ensure_ascii=False),
            "original_template": current_template,
        })
        current_template = new_template

        # 2. ê²€ì¦ í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ì§ì ‘ êµì • (ìŠ¤íŠ¸ë¦¬ë°)
        yield ("status", {"step": f"êµì •ëœ ë‹µë³€ ìž¬ìƒì„± ì¤‘...", "icon": "âœï¸"})
        yield ("token", f"\n\n---\nðŸ”„ **ìžë™ êµì •ëœ ë‹µë³€ ({round_num}íšŒì°¨):**\n\n")
        
        correction_llm = _get_llm(
            model=gen_model,
            temperature=0.1,
            api_key=openai_api_key,
            streaming=True,
        )
        correction_prompt = PromptTemplate.from_template(_CORRECTION_PROMPT_TEMPLATE)
        correction_chain = correction_prompt | correction_llm | StrOutputParser()
        
        current_answer = ""
        async for chunk in correction_chain.astream({
            "question": query,
            "context": context_text,
            "answer": last_answer_for_correction,
            "verify_result": last_verify_result,
        }):
            if chunk:
                current_answer += chunk
                yield ("token", chunk)
        
        last_answer_for_correction = current_answer

        # 3. ìž¬ê²€ì¦ (GPT-5.2)
        yield ("status", {"step": f"êµì • ë‹µë³€ ê²€ì¦ ì¤‘...", "icon": "ðŸ§"})
        last_verify_result = verify_answer(
            query, context_text, current_answer, openai_api_key, model="gpt-5.2"
        )

        # 4. RAGAS ì§€í‘œ ìƒëžµ (ì†ë„ë¥¼ ìœ„í•´ êµì • ë£¨í”„ ì¤‘ì—ëŠ” ì¸¡ì •í•˜ì§€ ì•ŠìŒ)
        # ìµœì¢… ê²°ê³¼ì—ì„œë§Œ 1íšŒ ì¸¡ì •í•˜ë„ë¡ api.pyì—ì„œ ì œì–´ ê¶Œìž¥
        last_ragas_result = {"faithfulness": 0.0, "answer_relevancy": 0.0}

        # ë¼ìš´ë“œ ë¡œê·¸ ê¸°ë¡
        correction_logs.append({
            "round": round_num,
            "answer": current_answer,
            "verify_result": last_verify_result,
            "ragas_result": last_ragas_result,
            "prompt_template": current_template,
        })

    # êµì • ì‹¤ì œ ìˆ˜í–‰ íšŸìˆ˜ ê³„ì‚°
    # FAILë¡œ ì‹œìž‘í•´ì„œ PASSë¡œ ëë‚¬ìœ¼ë©´ loopê°€ ëŒì•˜ìŒ.
    actual_rounds = round_num - 1
    if "PASS" in last_verify_result.upper() and actual_rounds == 0 and "FAIL" in initial_verify_result.upper():
        # ì´ ê²½ìš°ëŠ” ì´ë¡ ìƒ 1íšŒëŠ” ëŒì•„ì•¼ í•¨ (ìµœì†Œ 1íšŒ ì§„ìž… í›„ PASSê°€ ë˜ì—ˆìœ¼ë¯€ë¡œ)
        actual_rounds = 1
    
    # ë” ì •í™•í•œ ê³„ì‚°: logs ê¸¸ì´ë¥¼ í™œìš© (ì´ˆê¸° ë¡œê·¸ 1ê°œ + ë¼ìš´ë“œë³„ 1ê°œ)
    actual_rounds = len(correction_logs) - 1

    yield ("done_loop", {
        "answer": current_answer,
        "verify_result": last_verify_result,
        "rounds": actual_rounds,
        "logs": correction_logs,
        "ragas": last_ragas_result
    })


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RAGAS í‰ê°€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def evaluate_with_ragas(
    query: str,
    answer: str,
    final_docs: list[Document],
    embeddings,
    openai_api_key: str,
    eval_model: str = "gpt-5.2",
) -> dict[str, float]:
    """
    RAGASë¡œ RAG íŒŒì´í”„ë¼ì¸ì˜ faithfulnessì™€ answer_relevancyë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
    answer_relevancy ì •í™•ë„ë¥¼ ìœ„í•´ OpenAI ìž„ë² ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    os.environ["OPENAI_API_KEY"] = openai_api_key
    eval_llm = ChatOpenAI(model=eval_model)

    # answer_relevancy ë©”íŠ¸ë¦­ì€ ìž„ë² ë”© cosine similarityë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ
    # ë¡œì»¬ BGE-M3-ko ëŒ€ì‹  OpenAI text-embedding-3-small ì‚¬ìš©
    from langchain_openai import OpenAIEmbeddings
    ragas_embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # LLMê³¼ ë™ì¼í•œ ë¬¸ë§¥ ì‚¬ìš©: ì „ì²´ ë¬¸ì„œ, ë¬¸ì„œë‹¹ 1000ìž (build_contextì™€ ë™ì¼)
    ragas_data = {
        "question": [query],
        "answer": [answer],
        "contexts": [
            [d.page_content.replace("passage: ", "")[:1000] for d in final_docs]
        ],
    }
    logger.info("[RAGAS] Input - question: %s", query[:50])
    logger.info("[RAGAS] Input - answer length: %d, preview: %s", len(answer), answer[:100])
    logger.info("[RAGAS] Input - contexts count: %d", len(final_docs))
    dataset = Dataset.from_dict(ragas_data)

    try:
        results = _retry_api_call(
            lambda p: evaluate(
                dataset=p["dataset"],
                metrics=p["metrics"],
                llm=p["llm"],
                embeddings=p["embeddings"],
                raise_exceptions=False,
            ),
            {
                "dataset": dataset,
                "metrics": [faithfulness, answer_relevancy],
                "llm": eval_llm,
                "embeddings": ragas_embeddings,
            }
        )
        df = results.to_pandas()
        logger.info("[RAGAS] Evaluation successful. Columns: %s", df.columns.tolist())
        
        # ì»¬ëŸ¼ëª… ìœ ì—°í•˜ê²Œ ì°¾ê¸° (ë²„ì „ì— ë”°ë¼ 'faithfulness' ë˜ëŠ” 'faithfulness.score' ë“±ì¼ ìˆ˜ ìžˆìŒ)
        def _get_metric_val(keywords):
            for col in df.columns:
                if any(k.lower() in col.lower() for k in keywords):
                    return df.iloc[0][col]
            return 0.0

        f_val = _get_metric_val(["faithfulness"])
        r_val = _get_metric_val(["relevancy", "relevance"])

        import math
        logger.info("[RAGAS] Raw values: faithfulness=%s (type=%s), answer_relevancy=%s (type=%s)",
                     f_val, type(f_val).__name__, r_val, type(r_val).__name__)

        def _safe(val: float) -> float:
            try:
                v = float(val)
                return 0.0 if (math.isnan(v) or math.isinf(v)) else max(0.0, min(1.0, v))
            except Exception:
                return 0.0

        result = {
            "faithfulness": _safe(f_val),
            "answer_relevancy": _safe(r_val),
        }
        logger.info("[RAGAS] Final scores: %s", result)
        return result
    except Exception as e:
        logger.error("[RAGAS] Evaluation Error: %s", e)
        return {"faithfulness": 0.0, "answer_relevancy": 0.0}
