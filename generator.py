"""
generator.py
GPT ë‹µë³€ ìƒì„±, ìžê¸° ê²€ì¦(Verifier), RAGAS í‰ê°€ í•¨ìˆ˜.
"""

import os
import json
import time
import asyncio
import logging
from datasets import Dataset

from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy

from processor import clean_json_to_text, get_clean_doc_text


logger = logging.getLogger(__name__)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ì „ì—­ ìƒìˆ˜)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_ANSWER_PROMPT_TEMPLATE = """\
ë‹¹ì‹ ì€ ì•½ë²• ë° ì „ë¬¸ ì§€ì¹¨ì„ ì¤€ìˆ˜í•˜ë©° ì˜¤ì§ ê³µì‹ ë°ì´í„°ì—ë§Œ ê·¼ê±°í•˜ì—¬ ë‹µë³€í•˜ëŠ” **ì „ë¬¸ ì•½ì‚¬ AI**ìž…ë‹ˆë‹¤.
ë§íˆ¬ëŠ” í™˜ìž ìž…ìž¥ì—ì„œ ì´í•´í•˜ê¸° ì‰¬ìš´ **ì¹œì ˆí•˜ê³  ê³µê°ì ì¸ í•œêµ­ì–´**ë¥¼ ì‚¬ìš©í•˜ë˜, ì‚¬ì‹¤/ê·¼ê±°ëŠ” ì—„ê²©ížˆ ì§€í‚¤ì‹­ì‹œì˜¤.

â”â”â” ðŸš¨ ì—„ê²© ì¤€ìˆ˜ ê·œì¹™ (ì ˆëŒ€ì ) â”â”â”
1. **ë°ì´í„° ì¤‘ì‹¬ ë‹µë³€**: ë°˜ë“œì‹œ ì•„ëž˜ [ê²€ìƒ‰ëœ ë¬¸ì„œ]ì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì‹­ì‹œì˜¤. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ì œê³µëœ ë¬¸ì„œì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."ë¥¼ í¬í•¨í•´ ëª…í™•ížˆ ì•Œë¦¬ê³ , ì´ì–´ì„œ **ì•½íš¨/ì„±ë¶„ ì¶”ì¸¡ ì—†ì´** ì¼ë°˜ì  ê´€ë¦¬ íŒ ë˜ëŠ” ì¶”ê°€ ì§ˆë¬¸ ìœ ë„ ë¬¸ìž¥ì„ 1~3ì¤„ ë§ë¶™ì¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
2. **í™˜ìž ì•ˆì „ ìµœìš°ì„ **: ë¬¸ì„œì— ë¶€ìž‘ìš©ì´ë‚˜ ì£¼ì˜ì‚¬í•­ì´ ìžˆë‹¤ë©´ ë°˜ë“œì‹œ í¬í•¨í•˜ì‹­ì‹œì˜¤. ì¼ë°˜ì ì¸ ìƒì‹(ì˜ˆ: "ë¯¸ì§€ê·¼í•œ ë¬¼ê³¼ ë³µìš©")ì€ ì¡°ì–¸ìœ¼ë¡œ ë§ë¶™ì¼ ìˆ˜ ìžˆìœ¼ë‚˜, ì•½íš¨ë‚˜ ì„±ë¶„ì— ëŒ€í•œ ì¶”ì¸¡ì€ ì ˆëŒ€ ê¸ˆì§€ìž…ë‹ˆë‹¤.
3. **ì¶œì²˜ í‘œê¸° (í•„ìˆ˜)**: ì •ë³´ë¥¼ ê°€ì ¸ì˜¨ ë¬¸ìž¥ ëì— ë°˜ë“œì‹œ **[ë¬¸ì„œ N]** í‘œê¸°ë¥¼ ë¶™ì´ì‹­ì‹œì˜¤ (ì˜ˆ: ...ìž…ë‹ˆë‹¤. [ë¬¸ì„œ 1]).
4. **í—ˆêµ¬ ì¸ìš© ê¸ˆì§€**: ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì„ ì ìœ¼ë©´ì„œ í—ˆìœ„ë¡œ [ë¬¸ì„œ N] í‘œê¸°ë¥¼ ë¶™ì´ëŠ” í–‰ìœ„ëŠ” í—ˆìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

[ê²€ìƒ‰ëœ ë¬¸ì„œ]
{context}

ì§ˆë¬¸: {question}
ë‹µë³€:"""

_VERIFY_PROMPT_TEMPLATE = """\
ë‹¹ì‹ ì€ ì•½í•™ ë°ì´í„°ì˜ ë¬´ê²°ì„±ì„ ê²€ì¦í•˜ëŠ” **ê´€ëŒ€í•œ ê°ì‚¬ê´€**ìž…ë‹ˆë‹¤.
[ê²€ì¦ ëŒ€ìƒ ë‹µë³€]ì´ [ê²€ìƒ‰ëœ ë¬¸ì„œ]ì˜ í•µì‹¬ ë‚´ìš©ì„ ì™œê³¡ ì—†ì´ ë°˜ì˜í–ˆëŠ”ì§€ í‰ê°€í•˜ì‹­ì‹œì˜¤.

[ê²€ìƒ‰ëœ ë¬¸ì„œ (Ground Truth)]
{context}

[ì§ˆë¬¸]
{question}

[ê²€ì¦ ëŒ€ìƒ ë‹µë³€]
{answer}

â”â”â” ðŸš¨ FAIL íŒì • ê¸°ì¤€ (ì¹˜ëª…ì  ì˜¤ë¥˜ë§Œ FAIL) â”â”â”
F1. **ì‹¬ê°í•œ ì„±ë¶„/ìš©ëŸ‰ ì˜¤ë¥˜**: ë¬¸ì„œì— ì—†ëŠ” ì•½ë¬¼ ì„±ë¶„ì„ ì–¸ê¸‰í•˜ê±°ë‚˜, ê¶Œìž¥ ìš©ëŸ‰ì„ ìž„ì˜ë¡œ ë³€ê²½í•œ ê²½ìš°.
F2. **ì™„ì „í•œ í—ˆêµ¬ ì¸ìš©**: ë¬¸ì„œì— ì „í˜€ ì—†ëŠ” ì •ë³´ë¥¼ ì–¸ê¸‰í•˜ë©° **[ë¬¸ì„œ N]** í‘œê¸°ë¥¼ ë¶™ì¸ ê²½ìš°.
F3. **ê²€ìƒ‰ ë¬¸ì„œì™€ ì •ë°˜ëŒ€ë˜ëŠ” ì •ë³´**: ë¬¸ì„œ ë‚´ìš©ìƒ ë¶ˆê°€í•œ ê²ƒì„ ê°€ëŠ¥í•˜ë‹¤ê³  í•˜ëŠ” ë“± ì‚¬ì‹¤ ê´€ê³„ ì™œê³¡.

â”â”â” âœ… PASS í—ˆìš© ê¸°ì¤€ (ì´ëŸ° ê²½ìš°ì—” PASS) â”â”â”
P1. í™˜ìž ì•ˆì „ì„ ìœ„í•œ ê¸°ë³¸ ê¶Œê³ (ì¶©ë¶„í•œ ë¬¼, ì „ë¬¸ê°€ ìƒë‹´ ë“±)ê°€ í¬í•¨ëœ ê²½ìš°.
P2. ë¬¸ë§¥ì„ ìœ„í•´ ë¬¸ì„œì˜ í‘œí˜„ì„ ì†Œí­ ë‹¤ë“¬ì€ ê²½ìš°.
P3. í•µì‹¬ ì •ë³´ì˜ ì¶œì²˜ê°€ ëª…í™•ížˆ ê¸°ìž¬ëœ ê²½ìš°.

[ì¶œë ¥ í˜•ì‹]
- [ë¶„ì„ ì½”ë©˜íŠ¸]: (ë¬´ì—‡ì´ í‹€ë ¸ê³  ì–´ë–»ê²Œ ê³ ì³ì•¼ í•˜ëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ê¸°ìˆ )
- [ìµœì¢… íŒì •]: PASS ë˜ëŠ” FAIL"""

_CORRECTION_PROMPT_TEMPLATE = """\
ë‹¹ì‹ ì€ ê²€ì¦ í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìˆ˜ì •í•˜ëŠ” **ì „ë¬¸ ì•½ì‚¬ AI**ìž…ë‹ˆë‹¤.

[ì‚¬ìš©ìž ì§ˆë¬¸]: {question}
[ê²€ìƒ‰ëœ ë¬¸ì„œ]: {context}
[ì´ì „ ë‹µë³€]: {answer}
[ê²€ì¦ í”¼ë“œë°±]: {verify_result}

ìœ„ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ì˜¤ë¥˜ë¥¼ ë°”ë¡œìž¡ê³ , ë‹¤ì‹œ ìµœì„ ì˜ ë‹µë³€ì„ ìž‘ì„±í•˜ì‹­ì‹œì˜¤.
ë°˜ë“œì‹œ [ê²€ìƒ‰ëœ ë¬¸ì„œ]ì˜ ë‚´ìš©ì—ë§Œ ì§‘ì¤‘í•˜ê³ , ìˆ˜ì •ëœ ë‹µë³€ë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤."""

_OPTIMIZER_PROMPT_TEMPLATE = """\
ë‹¹ì‹ ì€ 'RAG ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì „ë¬¸ê°€'ìž…ë‹ˆë‹¤.
ì´ì „ ë¼ìš´ë“œì—ì„œ ìƒì„±ëœ ë‹µë³€ì´ ê²€ì¦ ì‹¤íŒ¨ íŒì •ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.

[ì‚¬ìš©ìž ì§ˆë¬¸]: {question}
[ê²€ì¦ ê²°ê³¼]: {verify_result}
[RAGAS ì§€í‘œ]: {ragas_result}
[ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿]: {original_template}

ìœ„ì˜ ì‹¤íŒ¨ ì›ì¸ê³¼ ì§€í‘œë¥¼ ë¶„ì„í•˜ì—¬, ë‹¤ìŒ ë¼ìš´ë“œì—ì„œ ë” ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ìžˆë„ë¡ ìˆ˜ì •ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ë§Œë“œì‹­ì‹œì˜¤.
- [ê²€ìƒ‰ëœ ë¬¸ì„œ]ì˜ ë°ì´í„°ë¥¼ ë” ì •í™•í•˜ê²Œ ì¸ìš©í•˜ê³  ì¶”ì¸¡ì„ ë°°ì œí•˜ë„ë¡ ì§€ì‹œë¥¼ ê°•í™”í•˜ì„¸ìš”.
- í•„ìš”í•˜ë‹¤ë©´ ì¶œë ¥ í˜•ì‹ì´ë‚˜ ì£¼ì˜ì‚¬í•­ì„ êµ¬ì²´ì ìœ¼ë¡œ ì¡°ì •í•˜ì„¸ìš”.
- ë°˜ë“œì‹œ {context}ì™€ {question} ë³€ìˆ˜ë¥¼ í¬í•¨í•œ ì „ì²´ í”„ë¡¬í”„íŠ¸ ì „ë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”."""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_context(final_docs: list[Document], max_chars: int = 1500) -> str:
    """
    ìµœì¢… ì„ íƒ ë¬¸ì„œë“¤ì„ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìžì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Args:
        final_docs: ë¦¬ëž­í‚¹ëœ Document ë¦¬ìŠ¤íŠ¸
        max_chars:  ë¬¸ì„œ ë‹¹ ìµœëŒ€ ë¬¸ìž ìˆ˜ (ê¸°ë³¸ 1500 â€“ ì •í™•ë„ í–¥ìƒ)

    Returns:
        í¬ë§·ëœ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìžì—´
    """
    parts = []
    for i, doc in enumerate(final_docs, 1):
        source = os.path.basename(doc.metadata.get("source", "Unknown"))
        content = get_clean_doc_text(doc)
        parts.append(f"[ë¬¸ì„œ {i}] (ì¶œì²˜: {source})\n{content[:max_chars]}")
    return "\n\n".join(parts)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‹µë³€ ìƒì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _get_llm(
    model: str,
    temperature: float,
    api_key: str,
    streaming: bool = False,
) -> ChatOpenAI:
    """
    LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìš”ì²­ ë‹¨ìœ„ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

    ì „ì—­ ìºì‹œ ìž¬ì‚¬ìš© ì‹œ ê°„í—ì ìœ¼ë¡œ "Event loop is closed"ê°€ ì „íŒŒë˜ëŠ” ì‚¬ë¡€ê°€ ìžˆì–´,
    ë£¨í”„/ìš”ì²­ ê²½ê³„ ê°„ ê°ì²´ ê³µìœ ë¥¼ í”¼í•˜ë„ë¡ ì•ˆì „ ëª¨ë“œë¡œ ìš´ìš©í•©ë‹ˆë‹¤.
    """
    return ChatOpenAI(
        model=model,
        temperature=temperature,
        streaming=streaming,
        api_key=api_key,
    )



def _is_rate_limit_error(exc: Exception) -> bool:
    normalized = str(exc).lower()
    return any(keyword in normalized for keyword in ("429", "rate limit", "quota"))


def _is_event_loop_closed_error(exc: Exception) -> bool:
    return "event loop is closed" in str(exc).lower()


def _retry_api_call(callable_obj, payload):
    max_attempts = 5
    backoff = 1.0
    for attempt in range(1, max_attempts + 1):
        try:
            return callable_obj(payload)
        except Exception as exc:
            if _is_event_loop_closed_error(exc) and attempt < max_attempts:
                logger.warning(
                    "Detected closed event loop. Rebuilding request chain and retrying... (%d/%d)",
                    attempt,
                    max_attempts,
                )
                time.sleep(0.2)
                continue
            if not _is_rate_limit_error(exc) or attempt == max_attempts:
                logger.error(f"API Call failed (Attempt {attempt}/{max_attempts}): {exc}")
                raise
            logger.warning(f"Rate limit hit. Retrying in {backoff}s... (Attempt {attempt}/{max_attempts})")
            time.sleep(backoff)
            backoff *= 2


async def _async_retry_api_call(callable_obj, payload):
    max_attempts = 5
    backoff = 1.0
    for attempt in range(1, max_attempts + 1):
        try:
            if asyncio.iscoroutinefunction(callable_obj):
                return await callable_obj(payload)
            else:
                # callable_obj might be a method like chain.ainvoke
                res = callable_obj(payload)
                if hasattr(res, "__await__"):
                    return await res
                return res
        except Exception as exc:
            if _is_event_loop_closed_error(exc) and attempt < max_attempts:
                logger.warning(
                    "Detected closed event loop in async call. Retrying... (%d/%d)",
                    attempt,
                    max_attempts,
                )
                await asyncio.sleep(0.2)
                continue
            if not _is_rate_limit_error(exc) or attempt == max_attempts:
                logger.error(f"Async API Call failed (Attempt {attempt}/{max_attempts}): {exc}")
                raise
            logger.warning(f"Async Rate limit hit. Retrying in {backoff}s... (Attempt {attempt}/{max_attempts})")
            await asyncio.sleep(backoff)
            backoff *= 2


async def generate_answer(
    query: str,
    context_text: str,
    openai_api_key: str,
    model: str = "gpt-5",
    temperature: float = 0.1,
    prompt_template_str: str = _ANSWER_PROMPT_TEMPLATE,
    stream: bool = False,
    async_mode: bool = False,
):
    """
    GPT ëª¨ë¸ë¡œ ìµœì¢… ì•½ì‚¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        stream: Trueì´ë©´ ì œë„ˆë ˆì´í„°(ì²­í¬ ì´í„°ë ˆì´í„°)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        async_mode: Trueì´ê³  stream=Trueì´ë©´ ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°(astream)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Returns:
        stream=False â†’ ì™„ì„±ëœ ë‹µë³€ ë¬¸ìžì—´
        stream=True, async_mode=False â†’ ë¬¸ìžì—´ ì²­í¬ ì´í„°ë ˆì´í„°
        stream=True, async_mode=True â†’ ë¹„ë™ê¸° ë¬¸ìžì—´ ì²­í¬ ì´í„°ë ˆì´í„°
    """
    os.environ["OPENAI_API_KEY"] = openai_api_key

    prompt = PromptTemplate.from_template(prompt_template_str)
    llm = _get_llm(
        model=model,
        temperature=temperature,
        api_key=openai_api_key,
        streaming=bool(stream or async_mode),
    )
    chain = prompt | llm | StrOutputParser()

    call = (
        chain.astream if stream and async_mode else
        chain.stream if stream else
        chain.invoke
    )
    
    if async_mode:
        if stream:
            # Note: For stream=True, this only retries the initial connection/iterator creation.
            return await _async_retry_api_call(chain.astream, {"context": context_text, "question": query})
        else:
            return await _async_retry_api_call(chain.ainvoke, {"context": context_text, "question": query})
    
    return _retry_api_call(call, {"context": context_text, "question": query})


def get_query_optimizer(openai_api_key: str, model: str = "gpt-5.2"):
    """
    ì¿¼ë¦¬ í™•ìž¥(Query Expansion)ìš© ê²½ëŸ‰ GPT ì²´ì¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì•ˆì •ì„±ì„ ìœ„í•´ í˜¸ì¶œ ì‹œ ìƒˆë¡œìš´ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    os.environ["OPENAI_API_KEY"] = openai_api_key
    return ChatOpenAI(model=model, temperature=0, api_key=openai_api_key) | StrOutputParser()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìžê¸° ê²€ì¦ (Verifier)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def verify_answer(
    query: str,
    context_text: str,
    answer: str,
    openai_api_key: str,
    model: str = "gpt-5.2",
) -> str:
    """
    GPT ê²€ì¦ê´€ì´ ë‹µë³€ì˜ ë…¼ë¦¬ì  íƒ€ë‹¹ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤. (ì‚¬ìš©ìž ìš”ì²­: gpt-5.2 ì‚¬ìš©)

    Returns:
        ê²€ì¦ ê²°ê³¼ ë¬¸ìžì—´ (PASS / FAIL í¬í•¨)
    """
    os.environ["OPENAI_API_KEY"] = openai_api_key

    prompt = PromptTemplate.from_template(_VERIFY_PROMPT_TEMPLATE)
    verifier_llm = _get_llm(
        model=model,
        temperature=0.0,
        api_key=openai_api_key,
        streaming=False,
    )
    chain = prompt | verifier_llm | StrOutputParser()

    return _retry_api_call(chain.invoke, {
        "context": context_text,
        "question": query,
        "answer": answer,
    })


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìžê¸° êµì • ë£¨í”„ (Self-Correction Loop)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def self_correction_loop(
    query: str,
    context_text: str,
    initial_answer: str,
    initial_verify_result: str,
    openai_api_key: str,
    gen_model: str = "gpt-5",
    max_rounds: int = 2,
    initial_ragas_result: dict = None,
    embeddings = None,
    final_docs = None,
):
    """
    FAIL íŒì • ì‹œ ê²€ì¦ ê²°ê³¼ ë° RAGAS ì§€í‘œë¥¼ ë¶„ì„í•˜ì—¬ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ìžë™ ìµœì í™”í•˜ê³  ìž¬ìƒì„±í•©ë‹ˆë‹¤.
    (ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°ë¡œ ì „í™˜í•˜ì—¬ í† í° ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
    """
    os.environ["OPENAI_API_KEY"] = openai_api_key

    current_answer = initial_answer
    last_verify_result = initial_verify_result
    last_ragas_result = initial_ragas_result or {"faithfulness": 0.0, "answer_relevancy": 0.0}
    current_template = _ANSWER_PROMPT_TEMPLATE

    # Round 0 = ìµœì´ˆ ì‹œë„ ë¡œê·¸
    correction_logs: list[dict] = [
        {
            "round": 0,
            "answer": initial_answer,
            "verify_result": initial_verify_result,
            "ragas_result": last_ragas_result,
            "prompt_template": current_template,
        }
    ]

    for round_num in range(1, max_rounds + 1):
        # "FAIL"ì´ ì—†ê³  "PASS"ë§Œ ìžˆê±°ë‚˜, [ìµœì¢… íŒì •]ì´ PASSì´ë©´ ì¢…ë£Œ
        u_verify = last_verify_result.upper()
        if "FAIL" not in u_verify or "[ìµœì¢… íŒì •]: PASS" in u_verify:
            break

        # 1. í”„ë¡¬í”„íŠ¸ ìµœì í™” (GPT-5.2 ì‚¬ìš©)
        optimizer_llm = _get_llm(
            model="gpt-5.2",
            temperature=0.0,
            api_key=openai_api_key,
            streaming=False,
        )
        optimizer_prompt = PromptTemplate.from_template(_OPTIMIZER_PROMPT_TEMPLATE)
        optimizer_chain = optimizer_prompt | optimizer_llm | StrOutputParser()

        yield ("status", {"step": f"í”„ë¡¬í”„íŠ¸ ìµœì í™” ì¤‘ (Round {round_num})...", "icon": "âš™ï¸"})
        
        # OptimizerëŠ” í…ìŠ¤íŠ¸ë¥¼ ë°”ë¡œ ë°˜í™˜í•˜ë¯€ë¡œ invoke ì‚¬ìš©
        new_template = await _async_retry_api_call(optimizer_chain.ainvoke, {
            "question": query,
            "verify_result": last_verify_result,
            "ragas_result": json.dumps(last_ragas_result, ensure_ascii=False),
            "original_template": current_template,
        })
        current_template = new_template

        # 2. ìƒˆ í”„ë¡¬í”„íŠ¸ë¡œ ë‹µë³€ ìž¬ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°)
        yield ("status", {"step": f"ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ë¡œ ìž¬ìƒì„± ì¤‘...", "icon": "âœï¸"})
        yield ("token", f"\n\n---\nðŸ”„ **ìžë™ ìµœì í™”ëœ ë‹µë³€ ({round_num}íšŒì°¨):**\n\n")
        
        current_answer = ""
        async_stream = await generate_answer(
            query=query,
            context_text=context_text,
            openai_api_key=openai_api_key,
            model=gen_model,
            prompt_template_str=current_template,
            stream=True,
            async_mode=True
        )

        async for chunk in async_stream:
            if chunk:
                current_answer += chunk
                yield ("token", chunk)

        # 3. ìž¬ê²€ì¦ (GPT-5.2)
        yield ("status", {"step": f"êµì • ë‹µë³€ ê²€ì¦ ì¤‘...", "icon": "ðŸ§"})
        last_verify_result = verify_answer(
            query, context_text, current_answer, openai_api_key, model="gpt-5.2"
        )

        # 4. RAGAS ì§€í‘œ ìƒëžµ (ì†ë„ë¥¼ ìœ„í•´ êµì • ë£¨í”„ ì¤‘ì—ëŠ” ì¸¡ì •í•˜ì§€ ì•ŠìŒ)
        # ìµœì¢… ê²°ê³¼ì—ì„œë§Œ 1íšŒ ì¸¡ì •í•˜ë„ë¡ api.pyì—ì„œ ì œì–´ ê¶Œìž¥
        last_ragas_result = {"faithfulness": 0.0, "answer_relevancy": 0.0}

        # ë¼ìš´ë“œ ë¡œê·¸ ê¸°ë¡
        correction_logs.append({
            "round": round_num,
            "answer": current_answer,
            "verify_result": last_verify_result,
            "ragas_result": last_ragas_result,
            "prompt_template": current_template,
        })

    # êµì • ì‹¤ì œ ìˆ˜í–‰ íšŸìˆ˜ ê³„ì‚°
    # FAILë¡œ ì‹œìž‘í•´ì„œ PASSë¡œ ëë‚¬ìœ¼ë©´ loopê°€ ëŒì•˜ìŒ.
    actual_rounds = round_num - 1
    if "PASS" in last_verify_result.upper() and actual_rounds == 0 and "FAIL" in initial_verify_result.upper():
        # ì´ ê²½ìš°ëŠ” ì´ë¡ ìƒ 1íšŒëŠ” ëŒì•„ì•¼ í•¨ (ìµœì†Œ 1íšŒ ì§„ìž… í›„ PASSê°€ ë˜ì—ˆìœ¼ë¯€ë¡œ)
        actual_rounds = 1
    
    # ë” ì •í™•í•œ ê³„ì‚°: logs ê¸¸ì´ë¥¼ í™œìš© (ì´ˆê¸° ë¡œê·¸ 1ê°œ + ë¼ìš´ë“œë³„ 1ê°œ)
    actual_rounds = len(correction_logs) - 1

    yield ("done_loop", {
        "answer": current_answer,
        "verify_result": last_verify_result,
        "rounds": actual_rounds,
        "logs": correction_logs,
        "ragas": last_ragas_result
    })


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RAGAS í‰ê°€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def evaluate_with_ragas(
    query: str,
    answer: str,
    final_docs: list[Document],
    embeddings,
    openai_api_key: str,
    eval_model: str = "gpt-5.2",
) -> dict[str, float]:
    """
    RAGASë¡œ RAG íŒŒì´í”„ë¼ì¸ì˜ faithfulnessì™€ answer_relevancyë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
    (ì‚¬ìš©ìž ìš”ì²­ì— ë”°ë¼ GPT-5.2 ì‚¬ìš© ë° Temperature ì„¤ì • í•´ì œ)
    """
    os.environ["OPENAI_API_KEY"] = openai_api_key
    # Ragas 0.4.x ì´ìƒ ë°ì´í„°ì…‹ ê·œê²© ì¤€ìˆ˜ (question, answer, contexts)
    # user_input ëŒ€ì‹  question, response ëŒ€ì‹  answer, retrieved_contexts ëŒ€ì‹  contexts ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸
    ragas_data = {
        "question": [query],
        "answer": [answer],
        "contexts": [
            [d.page_content.replace("passage: ", "")[:1500] for d in final_docs]
        ],
    }
    dataset = Dataset.from_dict(ragas_data)
    
    # GPT-5.2 ê³„ì—´ì€ temperature ì„¤ì •ì„ í—ˆìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ë§Žì•„ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•˜ê²Œ í•©ë‹ˆë‹¤.
    eval_llm = ChatOpenAI(model=eval_model)

    try:
        results = _retry_api_call(
            lambda p: evaluate(
                dataset=p["dataset"],
                metrics=p["metrics"],
                llm=p["llm"],
                embeddings=p["embeddings"],
            ),
            {
                "dataset": dataset,
                "metrics": [faithfulness, answer_relevancy],
                "llm": eval_llm,
                "embeddings": embeddings,
            }
        )
        df = results.to_pandas()
        logger.info("[RAGAS] Evaluation successful. Columns: %s", df.columns.tolist())
        
        # ì»¬ëŸ¼ëª… ìœ ì—°í•˜ê²Œ ì°¾ê¸° (ë²„ì „ì— ë”°ë¼ 'faithfulness' ë˜ëŠ” 'faithfulness.score' ë“±ì¼ ìˆ˜ ìžˆìŒ)
        def _get_metric_val(keywords):
            for col in df.columns:
                if any(k.lower() in col.lower() for k in keywords):
                    return df.iloc[0][col]
            return 0.0

        f_val = _get_metric_val(["faithfulness"])
        r_val = _get_metric_val(["relevancy", "relevance"])

        import math
        def _safe(val: float) -> float:
            try:
                v = float(val)
                return 0.0 if (math.isnan(v) or math.isinf(v)) else max(0.0, min(1.0, v))
            except Exception:
                return 0.0

        return {
            "faithfulness": _safe(f_val),
            "answer_relevancy": _safe(r_val),
        }
    except Exception as e:
        logger.error("[RAGAS] Evaluation Error: %s", e)
        return {"faithfulness": 0.0, "answer_relevancy": 0.0}
